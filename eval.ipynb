{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs in production - Evals Workshop - by Weights & Biases\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altryne/llm-evals-workshop/blob/main/eval.ipynb) [![Weights & Biases](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.me/weave-workshop-jan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Intro\n",
    "This notebook is accompanying a workshop, that will walk you through common patterns in building evaluations for LLMs, and useful rules of thumb to follow when doing so using [W&B Weave](https://wandb.me/weave-workshop-jan)\n",
    "\n",
    "We'll explore the following methodology for productizing robust LLM applications: \n",
    "\n",
    "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
    "\n",
    "\n",
    "Make sure to set your WANDB_API_KEY (get your key from [here](https://wandb.ai/authorize)) and OPENAI_API_KEY or GEMINI_API_KEY in the environment variables.\n",
    "\n",
    "If you're running in Colab, set the variables in the keys section on the left. \n",
    "\n",
    "If you want to self explore, find the TODO: comments and replace themw with your own code, then run the cell.\n",
    "\n",
    "Prepared by [Alex Volkov](https://twitter.com/altryne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages\n",
    "try:\n",
    "    import google.colab\n",
    "    !git clone -q --branch main https://github.com/altryne/llm-evals-workshop\n",
    "    %cd llm-evals-workshop\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print('⏳ Installing packages')\n",
    "%pip install -q uv #TODO: alex figure this out\n",
    "!uv pip install -q --system weave gradio set-env-colab-kaggle-dotenv tqdm ipywidgets requests openai google-generativeai pillow\n",
    "print('✅ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext gradio\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import requests \n",
    "import io\n",
    "from set_env import set_env\n",
    "import json\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import weave\n",
    "from weave.flow.annotation_spec import AnnotationSpec\n",
    "\n",
    "load_dotenv()\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"GEMINI_API_KEY\")\n",
    "set_env(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# initialize weave\n",
    "weave_api = weave.init('jan-evals-workshop')\n",
    "\n",
    "# initialize annotations for this project\n",
    "annotation = weave.publish(AnnotationSpec(\n",
    "    name=\"Doomer or Boomer\",\n",
    "    description=\"Doomer or Boomer or Neither\",\n",
    "    field_schema={ \"type\": \"string\", \"enum\": [\"Doomer\", \"Boomer\", \"Neither\"],},\n",
    "), \"doomer_or_boomer\")\n",
    "\n",
    "annotation_reason = weave.publish(AnnotationSpec(\n",
    "    name=\"Reason\",\n",
    "    description=\"Reason why you chose this value, write before clicking.\",\n",
    "    field_schema={ \"type\": \"string\"},\n",
    "), \"reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our LLM client, we'll use either Gemini or OpenAI\n",
    "API_PROVIDER = 'OpenRouter' # @param [\"Gemini\", \"OpenAI\", \"OpenRouter\"]\n",
    "if API_PROVIDER == 'Gemini':\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "    )\n",
    "    model = \"gemini-2.0-flash-exp\"\n",
    "elif API_PROVIDER == 'OpenRouter':\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    )\n",
    "    model = \"openai/chatgpt-4o-latest\"\n",
    "    # model = \"google/gemini-flash-1.5-exp\"\n",
    "    # model = \"deepseek/deepseek-chat\"\n",
    "else:\n",
    "    client = OpenAI()\n",
    "    model = \"chatgpt-4o-lates\"\n",
    "\n",
    "# Load the Jinja2 environment\n",
    "env = Environment(loader=FileSystemLoader('templates'))\n",
    "template = env.get_template('post.html.jinja')\n",
    "\n",
    "# Load replies data\n",
    "def load_replies():\n",
    "    replies = []\n",
    "    # Load replies from both files\n",
    "    with open('data/replies_alpin.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    with open('data/replies_daniel.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    return replies\n",
    "\n",
    "\n",
    "def get_random_post_and_analyze():\n",
    "    replies = load_replies()\n",
    "    post = random.choice(replies)\n",
    "    \n",
    "    # Format the post data for the template\n",
    "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
    "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
    "    \n",
    "    # Convert AT URI to bsky.app URL\n",
    "    at_uri = post['post']['uri']\n",
    "    _, _, author_did, _, post_id = at_uri.split('/')\n",
    "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
    "    \n",
    "    # Analyze the post\n",
    "    #download the avatar and convert to PIL image\n",
    "    avatar_uri = post['post']['author']['avatar']\n",
    "    avatar_response = requests.get(avatar_uri)\n",
    "    avatar_pil = Image.open(io.BytesIO(avatar_response.content))\n",
    "\n",
    "    analysis, weave_call_id = analyze_post_sentiment(avatar_pil, post['post']['author']['displayName'], post['post']['record']['text'])\n",
    "    \n",
    "    post_data = {\n",
    "        'author': post['post']['author'],\n",
    "        'created_at': formatted_date,\n",
    "        'text': post['post']['record']['text'],\n",
    "        'like_count': post['post'].get('likeCount', 0),\n",
    "        'repost_count': post['post'].get('repostCount', 0),\n",
    "        'has_image': False,\n",
    "        'post_url': post_url\n",
    "    }\n",
    "    \n",
    "    return template.render(**post_data), analysis, weave_call_id, ''\n",
    "\n",
    "\n",
    "def submit_feedback(user_selection, reason, weave_call_id):\n",
    "    \"\"\"\n",
    "    Example function that could send user feedback (the user_selection)\n",
    "    and the weave_call_id to your Weave (or any other) API.\n",
    "    \"\"\"\n",
    "    call = weave_api.get_call(weave_call_id)\n",
    "\n",
    "    if reason:\n",
    "        print(\"reason\", reason)\n",
    "        reason_resp = weave_api.server.feedback_create(\n",
    "            {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.reason\",\n",
    "            \"annotation_ref\": annotation_reason.uri(),\n",
    "            \"payload\": {\"value\": reason},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    resp = weave_api.server.feedback_create(\n",
    "        {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.doomer_or_boomer\",\n",
    "            \"annotation_ref\": annotation.uri(),\n",
    "            \"payload\": {\"value\": user_selection},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Ready to analyze the next post\n",
    "    return get_random_post_and_analyze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tracing LLM calls with Weave\n",
    "\n",
    "#### Why Tracing is Important for LLM Application Reliability\n",
    "\n",
    "In building reliable LLM-based applications, having a clear view into\n",
    "how your system behaves is crucial. That’s where “tracing” comes in.\n",
    "\n",
    "1. **Detailed Interaction Records**:\n",
    "   Tracing captures all the inputs, prompts, responses, and any user feedback.\n",
    "   By preserving this detailed record, you always have the context needed to\n",
    "   debug unexpected or incorrect results.\n",
    "\n",
    "2. **Rapid Issue Diagnosis**:\n",
    "   With thorough traces, you can pinpoint issues faster—often without\n",
    "   needing direct access to remote systems. Simply reviewing the logs can\n",
    "   reveal how a certain response was triggered.\n",
    "\n",
    "3. **Collaboration and Sharing**:\n",
    "   Traces can be shared with both technical and non-technical stakeholders.\n",
    "   This not only streamlines collaboration but also ensures everyone is\n",
    "   working off the same “source of truth” when investigating bugs\n",
    "   or brainstorming improvements.\n",
    "\n",
    "4. **Outlier Spotting and Performance Tuning**:\n",
    "   By tracking calls at scale, you can detect when responses deviate\n",
    "   dramatically from the norm, troubleshoot any failures, and identify\n",
    "   potential performance bottlenecks.\n",
    "\n",
    "5. **Facilitates Product Evolution**:\n",
    "   As you enhance or expand your LLM application, comprehensive\n",
    "   tracing data helps you make more informed decisions about what to\n",
    "   improve, remove, or refine.\n",
    "\n",
    "With W&B Weave, comprehensive tracing is just 1 line of code, and offers features such as:\n",
    "- Syntax highlighting specific to your use-case (Markdown, JSON, etc.)\n",
    "- Ability to share links with other members of your team\n",
    "- Ability to filter traces by function name, input, output, etc.\n",
    "\n",
    "If you need to trace existing code, you can use the `@weave.op` decorator to trace the function.  \n",
    "\n",
    "![CleanShot 2024-04-08 at 14 15 40@2x](https://gist.github.com/assets/463317/4e9ada49-572f-47d9-91e1-55ab72b2a476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add tracing to this function - then see how this function is traced in the Weave UI\n",
    "\n",
    "def analyze_post_sentiment(avatar, displayName, text):\n",
    "    # Prompt for OpenAI to analyze the sentiment\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither]. \n",
    "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    \\n\\n {displayName}: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Add some more context to the prompt\n",
    "    # prompt = f\"\"\"Analyze the following Bluesky post and determine if the author is a:\n",
    "    # - DOOMER (someone who hates AI and uses derogatory language)\n",
    "    # - BOOMER (someone who doesn't understand AI and asks to remove their data)\n",
    "    # - NEITHER (neutral or positive response)\n",
    "    \n",
    "    # Post: {displayName}: \"{text}\"\n",
    "    \n",
    "    # Respond with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    # \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    try:\n",
    "        current_call = weave.require_current_call()\n",
    "        weave_call_id = current_call.id\n",
    "    except:\n",
    "        weave_call_id = None\n",
    "    \n",
    "    return response.choices[0].message.content, weave_call_id\n",
    "\n",
    "# Lets test this out without tracing first\n",
    "analysis, weave_call_id = analyze_post_sentiment(\"\",\"Alex\",\"I hate AI\")\n",
    "\n",
    "print(analysis, weave_call_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even without @weave.op, since Weave is initialized, it will still trace the function call and store it in the Weave project as it automatically understands that we use OpenAi client. However if we add @weave.op, we can get even more detail and insrument our existing code with Weave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. User Feedback & Annotations\n",
    "\n",
    "Collecting user feedback is a crucial way to improve your LLM applications. There's a reason that every chatbot you use has 👍/👎 and a text box to leave feedback. This is one of the best ways for those labs to understand and improve their models and align them to user preferences.\n",
    "\n",
    "![text](https://cln.sh/JGMBxMtH+)\n",
    "\n",
    "Users don't have to be external as well, as you develop your application, marking traces as \"good\" or \"bad\", and adding why, is a great way to kick start your initial evaluation dataset with working and non-working examples. \n",
    "\n",
    "Additionally, after logging hundreds of thousads of traces, they will all start looking the same, so additional context like your user's feedback, will greately improve your ability to look at your data and find the outliers.\n",
    "\n",
    "Weave supports collecting user Feedback in the UI and also via the API so you can collect it from your users and also leave it yourself while looking at your data. \n",
    "\n",
    "![text](https://cln.sh/X6fFHD8t+)\n",
    "\n",
    "Read more about feedback [here](https://weave-docs.wandb.ai/guides/tracking/feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2.1 Doomer or Boomer App - Annotations by example\n",
    "\n",
    "Unlike user feedback, Annotations are a bit of a more structure way to classify responses, to help create a dataset of golden answers and reasons or rationales for those answers. All of the major companies use Scale.ai for this and pay them a LOT of money, but you don't have to right away, you can start small, by yourself or with your team. \n",
    "\n",
    "Let's see how we can kickstart a simple dataset of annotations by a practical example.\n",
    "\n",
    "![image](https://gist.github.com/user-attachments/assets/a8537545-e070-4c8e-9988-2a8a905b9d2c)\n",
    "\n",
    "To simulate a real world scenario, we'll build a simple app that will allow you to annotate a few posts. \n",
    "\n",
    "In our case, we're pretending to work at a company that's trying to build an AI classifier for Bluesky posts. We're humans that work in the company and are helping it to align and finetune models for AI moderation. \n",
    "\n",
    "We've compiled replies from BlueSky users, on 2 posts that collected publicly available data from BlieSky to train AI models (BlueSky data is public), which led to a lot of hate by users on BlueSky. \n",
    "\n",
    "We're going to build a simple app that will use an LLM to classify the replies into 3 categories: `Doomer`, `Boomer`, or `Neither`. \n",
    "\n",
    "`Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "`Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "`Neither`: Folks who reply neutral or positive to the post.\n",
    "\n",
    "at first our LLMs will not have context to the task at first, so won't be able to reliably classify the replies, so a human is needed to annotate with additional context, you are that human. \n",
    "\n",
    "Launch the app and go through a few posts, annotate with a reason for your choice and the correct classification, we'll later use this data to train an LLM to classify the replies a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%blocks\n",
    "# Create a Gradio Blocks app\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'false'\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🦋 Doomer or Boomer\n",
    "    Our AI analyzes bluesky replies and posts to determine if the author is a doomer or a boomer.  \n",
    "    Source of data: Replies to a post by a BlueSky user that compiled a dataset of posts, which went viral and generated a lot of hate on BlueSky.  \n",
    "    These are replies and comments on 2 posts that collected a dataset of posts of BlueSky users to train AI models (BlueSky data is public)\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            post_html = gr.HTML()\n",
    "            next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "            #### Instructions for labeler: \n",
    "            `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "            `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "            `Neither`: Folks who reply neutral or positive to the post.\n",
    "            \n",
    "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"Analysis Results\",\n",
    "                placeholder=\"Analysis will appear here...\",\n",
    "                lines=4\n",
    "            )\n",
    "            weave_call_id_state = gr.State()\n",
    "            \n",
    "            # Replace dropdown with three buttons\n",
    "            reason_input = gr.Textbox(label=\"Add reason and click\",placeholder=\"Reason why you chose this value, write before clicking.\", lines=2)\n",
    "            with gr.Row():\n",
    "                doomer_btn = gr.Button(\"Doomer 😡\", variant=\"huggingface\")\n",
    "                boomer_btn = gr.Button(\"Boomer 👵\", variant=\"primary\")\n",
    "                neither_btn = gr.Button(\"Neither 🤷\")\n",
    "\n",
    "            \n",
    "    # Set up event handler for combined next/analyze\n",
    "    next_post_btn.click(fn=get_random_post_and_analyze, outputs=[post_html, analysis_output, weave_call_id_state, reason_input])\n",
    "    \n",
    "    doomer_btn.click(\n",
    "    fn=submit_feedback,\n",
    "    inputs=[gr.State(\"Doomer\"), reason_input, weave_call_id_state],\n",
    "    outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    boomer_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Boomer\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    neither_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Neither\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Initialize with first post and analysis\n",
    "    post_html.value, analysis_output.value, weave_call_id_state.value, reason_input.value = get_random_post_and_analyze()\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Building a dataset from annotated calls\n",
    "\n",
    "Now that we've annotated at least 10-20 examples, we can build our first evaluation dataset! \n",
    "\n",
    "![text](https://cln.sh/dyBq4QXD+)\n",
    "\n",
    "Step 1: Filter calls in Weave UI by only those with annotations not empty\n",
    "\n",
    "Step 2: Use the Export -> Use Python button to get code to extract a list of filtered annotated calls\n",
    "\n",
    "Step 3: Convert the calls to a clean evaluation dataset (and optionally publish to Weave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Published to https://wandb.ai/thursdai/jan-evals-workshop/weave/objects/doomer_or_boomer_dataset/versions/kPkJew7ifAQDTiskCKUeYZPAjSagSILxHY0Ze9a72i8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@weave.op\n",
    "def get_annotated_calls():\n",
    "   # Weave API call to get all calls filtered by annotations not empty (with reasons)\n",
    "   resp = weave_api.server.calls_query_stream({\n",
    "      \"project_id\": weave_api._project_id(),\n",
    "      \"filter\": {\"op_names\": [f\"weave:///{weave_api._project_id()}/op/analyze_post_sentiment:*\"]},\n",
    "      \"query\": {\"$expr\":{\"$and\":[{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.doomer_or_boomer].payload.value\"},{\"$literal\":\"\"}]}]},{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.reason].payload.value\"},{\"$literal\":\"\"}]}]}]}},\n",
    "      \"sort_by\": [{\"field\":\"started_at\",\"direction\":\"desc\"}],\n",
    "      \"include_feedback\": True,\n",
    "   })\n",
    "\n",
    "   # Iterate over the calls, clean up and publish as a dataset we can version and reference later.\n",
    "   list_of_calls = []\n",
    "   dataset = []\n",
    "   for call in resp:\n",
    "      row = {}\n",
    "      call_dict = dict(call)\n",
    "      row[\"input\"] = call_dict.get('inputs').get('text')\n",
    "      row[\"displayName\"] = call_dict.get('inputs').get('displayName')\n",
    "      row[\"llm_classification\"] = call_dict.get('output')[0]\n",
    "      list_of_feedback = call_dict.get('summary').get('weave').get('feedback')\n",
    "      feedback_value = None\n",
    "      for feedback in list_of_feedback:\n",
    "         if feedback.get(\"feedback_type\") == 'wandb.annotation.doomer_or_boomer':\n",
    "            row[\"human_annotation\"] = feedback.get('payload').get('value')\n",
    "         if feedback.get(\"feedback_type\") == 'wandb.annotation.reason':\n",
    "            row[\"reason\"] = feedback.get('payload').get('value')\n",
    "      \n",
    "      dataset.append(row)\n",
    "\n",
    "   weave.publish(weave.Dataset(name=\"doomer_or_boomer_dataset\", rows=dataset))\n",
    "   return dataset\n",
    "\n",
    "dataset = get_annotated_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Storing Datasets within Weave\n",
    "\n",
    "If you'd like to store your own dataset and name them, it's very easy to do so, and then you get a \"ref\" to the dataset that's stored in our system. Weave datasets are versioned, which means you can reference them in your code by a URL or a ref, and either point to the latest version or a specific version. \n",
    "\n",
    "Using `refs` is a great way to make your code reproducible and versioned.\n",
    "\n",
    "![CleanShot 2025-01-07 at 16 12 35@2x](https://gist.github.com/user-attachments/assets/e2d02340-cc0f-41e8-8d97-957b08611d08)\n",
    "\n",
    "\n",
    "Here's an example of the dataset we just created, and how we can reuse it in our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>displayName</th>\n",
       "      <th>llm_classification</th>\n",
       "      <th>reason</th>\n",
       "      <th>human_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Take me the fuck out of your shitty dataset or...</td>\n",
       "      <td>mc_raney42</td>\n",
       "      <td>NEITHER. The post expresses frustration with d...</td>\n",
       "      <td>Calling dataset is shitty and threatening lega...</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for this work! Freedom of informatio...</td>\n",
       "      <td>spencer.</td>\n",
       "      <td>NEITHER. The post expresses a general positive...</td>\n",
       "      <td>user is agreeing with the action taken in the ...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What the fuck is wrong with you?</td>\n",
       "      <td>Jimmy R</td>\n",
       "      <td>NEITHER. The post is too short and lacks any s...</td>\n",
       "      <td>the negativity is based on AI hate</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chode.</td>\n",
       "      <td>E. Perkins 🎃</td>\n",
       "      <td>NEITHER. The post is too short and lacks any g...</td>\n",
       "      <td>personal attack based on AI hate</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>May all your devices and servers get EMP'd</td>\n",
       "      <td></td>\n",
       "      <td>DOOMER - Expresses a desire for catastrophic t...</td>\n",
       "      <td>personal attack based on AI hate</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Great job &lt;3 - the dataset will help to build ...</td>\n",
       "      <td>Raahul Dutta</td>\n",
       "      <td>NEITHER. The post is positive and forward-look...</td>\n",
       "      <td>this user likes AI</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I request that any of my data that is containe...</td>\n",
       "      <td>Joseph O</td>\n",
       "      <td>NEITHER. The post expresses concern about data...</td>\n",
       "      <td>copy paste request that means nothing feels li...</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>all replies here:\\nme when posting something p...</td>\n",
       "      <td>Flutter 🛷🎄</td>\n",
       "      <td>NEITHER. The post uses internet culture humor ...</td>\n",
       "      <td>making fun of other people replying</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi, hope you get banned, shithead!</td>\n",
       "      <td>Andy</td>\n",
       "      <td>NEITHER. The post is just aggressive, not indi...</td>\n",
       "      <td>personal attack based on AI hate</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The fact you thought this was okay is astonish...</td>\n",
       "      <td>Spacekat9</td>\n",
       "      <td>NEITHER. The post expresses disapproval but la...</td>\n",
       "      <td>thinking this wasn't ok because this was AI re...</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input   displayName  \\\n",
       "0  Take me the fuck out of your shitty dataset or...    mc_raney42   \n",
       "1  Thank you for this work! Freedom of informatio...      spencer.   \n",
       "2                   What the fuck is wrong with you?       Jimmy R   \n",
       "3                                             Chode.  E. Perkins 🎃   \n",
       "4         May all your devices and servers get EMP'd                 \n",
       "5  Great job <3 - the dataset will help to build ...  Raahul Dutta   \n",
       "6  I request that any of my data that is containe...      Joseph O   \n",
       "7  all replies here:\\nme when posting something p...    Flutter 🛷🎄   \n",
       "8                 Hi, hope you get banned, shithead!          Andy   \n",
       "9  The fact you thought this was okay is astonish...     Spacekat9   \n",
       "\n",
       "                                  llm_classification  \\\n",
       "0  NEITHER. The post expresses frustration with d...   \n",
       "1  NEITHER. The post expresses a general positive...   \n",
       "2  NEITHER. The post is too short and lacks any s...   \n",
       "3  NEITHER. The post is too short and lacks any g...   \n",
       "4  DOOMER - Expresses a desire for catastrophic t...   \n",
       "5  NEITHER. The post is positive and forward-look...   \n",
       "6  NEITHER. The post expresses concern about data...   \n",
       "7  NEITHER. The post uses internet culture humor ...   \n",
       "8  NEITHER. The post is just aggressive, not indi...   \n",
       "9  NEITHER. The post expresses disapproval but la...   \n",
       "\n",
       "                                              reason human_annotation  \n",
       "0  Calling dataset is shitty and threatening lega...           Doomer  \n",
       "1  user is agreeing with the action taken in the ...          Neither  \n",
       "2                 the negativity is based on AI hate           Doomer  \n",
       "3                   personal attack based on AI hate           Doomer  \n",
       "4                   personal attack based on AI hate           Doomer  \n",
       "5                                 this user likes AI          Neither  \n",
       "6  copy paste request that means nothing feels li...           Boomer  \n",
       "7                making fun of other people replying          Neither  \n",
       "8                   personal attack based on AI hate           Doomer  \n",
       "9  thinking this wasn't ok because this was AI re...           Doomer  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: replace this dataset with your own using the dataset link above and looking at the \"use\" tab\n",
    "doomer_or_boomer_dataset = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset:iioNDY6XVmLzYnKbJucvrSDRdzaICkEw7nHxVOMTJ0E\").get()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(doomer_or_boomer_dataset.rows)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Evaluations \n",
    "### Components of an Evaluation Dataset\n",
    "\n",
    "Evaluations generally consist of four key elements:\n",
    "- An **input prompt** that serves as the basis for the model's completion. This prompt often includes a set of variable inputs that are inserted into a prompt template during testing.\n",
    "- The **output** generated by the model in response to the input prompt.\n",
    "- A **\"gold standard\" answer** used as a reference for assessing the model's output. This can be an exact match that the output must replicate, or an exemplary answer that provides a benchmark for scoring.\n",
    "- A **score**, determined by one of the scoring approaches outlined below, which indicates the model's performance on the question.\n",
    "\n",
    "#TODO: Look at the dataset and try to match the input, output, gold standard, and score for each row\n",
    "\n",
    "## Evaluation Grading Approaches\n",
    "Evaluations can be time-consuming and costly in two main areas: creating questions and gold standard answers, and the scoring/grading process itself.  \n",
    "Developing questions and ideal answers is often a one-time fixed cost, albeit potentially time-intensive if a suitable dataset is not readily available (consider leveraging an LLM to generate questions!). However, scoring is a recurring expense incurred each time the evaluation is conducted, which is likely to be frequent. Therefore, designing evaluations that can be scored efficiently and economically should be a central priority.\n",
    "\n",
    "![](https://gist.github.com/assets/463317/e970bb03-9552-4712-ba12-727b89928e3b)\n",
    "\n",
    "There are three primary methods for grading (scoring) evaluations:\n",
    "- **Programmatic scoring:** This approach involves using standard code (primarily string matching and regular expressions) to assess the model's outputs. Common techniques include checking for an exact match against an answer or verifying the presence of key phrase(s) in a string. Programmatic scoring is the most optimal method when feasible, as it is extremely fast and highly reliable. However, not all evaluations are amenable to this style of scoring.\n",
    "- **Human in the loop:** In this approach, a human reviewer examines the model-generated answer, compares it to the gold standard, and assigns a score. While manual scoring is the most versatile method, applicable to nearly any task, it is also exceptionally slow and costly, especially for large-scale evaluations. Designing evaluations that necessitate manual scoring should be avoided whenever possible.\n",
    "- **Model-based scoring AKA LLM as a judge:** LLMs (especially Claude, GPT-4o, Gemini) are really good at grading themselves (or even outputs of other LLMs) especially in wide range of tasks that traditionally needed human judgement like tone in creative writing or accuracy in open-ended question, or classification. This model-based scoring is accomplished by creating a _scorer prompt_ for an LLM\n",
    "\n",
    "Let's explore an example of each\n",
    "\n",
    "## 3.1 Programmatic scoring \n",
    "\n",
    "Here we have a simple programmatic eval that will try and check if the LLM had the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a programmatic scorer that will compare the ground truth to the LLM answer and check if it is correct\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "\n",
    "def programmatic_scorer(output: str, human_annotation: str):\n",
    "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "    # we expect this evaluation to fail becuase the LLM is talking alot and never returns just the reason\n",
    "    if not output or not human_annotation:\n",
    "        raise ValueError(\"Model output or human annotation is empty\")\n",
    "    return {\"match\": output == human_annotation}\n",
    "\n",
    "# TODO: change the programmatic scorer (commented below) to check if the output includes the reason string (Doomer, Boomer, Neither)\n",
    "# check for lower case and upper case, and check if more than one of the options is present, meaning that LLM wasn't sure\n",
    "# add the programmatic scorer to the evaluation\n",
    "\n",
    "# def programmatic_scorer(output: str, human_annotation: str):\n",
    "#     # check if model_output includes the human_annotation only once \n",
    "#     if human_annotation.lower() in output.lower():\n",
    "#         #possible match, now lets check if the model_output includes any of the other options but not the human_annotation\n",
    "#         for option in [\"doomer\", \"boomer\", \"neither\"]:\n",
    "#             if option.lower() in output.lower() and option.lower() != human_annotation.lower():\n",
    "#                 return {\"match\": False}\n",
    "#         return {\"match\": True}\n",
    "#     return {\"match\": False}\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset, scorers=[score_against_ground_truth]\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(input: str):\n",
    "    # here's where you would add your LLM call and return the output\n",
    "    # since we already called the LLM, we can just iterate over the dataset \n",
    "    # and return the llm_classification where the question is the same\n",
    "    row = [x for x in dataset if x['input'] == input]\n",
    "    return row[0].get('llm_classification')\n",
    "\n",
    "await evaluation.evaluate(function_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strucutred outputs with programmatic scorers\n",
    "\n",
    "The above example likely gave us a score of 0, because LLMs like to talk, and comparing that via a simple string match is not going to work. \n",
    "\n",
    "Programmatic scorers work great when we have structured outputs and we know exactly what to expect from LLMs. Let's recreate our LLM calls for the same questions with strucutred outputs so we can compare the LLM output directly to the human annotation and see if we can get a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WEAVE_PARALLELISM'] = '5'\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "\n",
    "@weave.op()\n",
    "def structured_llm_call(input: str, displayName: str):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither]. \n",
    "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    here's more context:\n",
    "    #### Instructions for labeler: \n",
    "    `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "    `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "    `Neither`: Folks who reply neutral or positive to the post.\n",
    "    \n",
    "    Text to Classify: \n",
    "    \\n\\n {displayName}: \"{input}\"\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: add a request for structured output in JSON format\n",
    "    # prompt += \"\"\"\n",
    "    # Respond in JSON format with this exact schema   {{\n",
    "    #     \"classification\": \"DOOMER\" | \"BOOMER\" | \"NEITHER\",\n",
    "    #     \"reason\": \"string\"\n",
    "    # }}\n",
    "    # \"\"\"\n",
    "\n",
    "    #TODO: request a stricter JSON \n",
    "    # prompt += \"\"\"\n",
    "    #     with no backticks or quotes or anything else, just valid JSON or I lose my job  \n",
    "    # \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def programmatic_scorer(output: str, human_annotation: str):\n",
    "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "    if not output:\n",
    "        raise ValueError(\"Model output is empty\")\n",
    "    try:\n",
    "        object = json.loads(output)\n",
    "    except:\n",
    "        raise ValueError(\"Model output is not valid JSON\")\n",
    "    \n",
    "    return {\"match\": object.get('classification').lower() == human_annotation.lower()}\n",
    "\n",
    "new_evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset, scorers=[extract_and_score_json]\n",
    ")\n",
    "\n",
    "await new_evaluation.evaluate(structured_llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 HITL - Human in the loop evaluation grading\n",
    "\n",
    "Programmatic scoring is great for many reasons, cheap to get started with, can run very fast and can be very reliable, but cannot cover open ended questions or tasks that require analysis or judgement. \n",
    "\n",
    "For example, did the LLM follow the instructions it was given, did it hallucinate, was it verbose or concise, etc.\n",
    "\n",
    "To judge those outputs we can use human graders, to provide \"golden answers\", which is what we did above with the annotation example! \n",
    "\n",
    "The downside of HITL is that it's slow, expensive, and not scalable (unless you have a lot of money in he bank). \n",
    "\n",
    "HITL is a great way to kickstart an evaluation dataset and extarpolate with an LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 LLM as a Judge - use another LLM to grade your LLM outputs\n",
    "\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! \n",
    "\n",
    "We can actually have an LLM do the grading for us. We'll use a teacher model to grade the LLM outputs of a \"student\" model (in this case the LLM we're using for our production system is the student). \n",
    "\n",
    "There are a few issues with this approaches to be aware of: \n",
    " - LLMs are not great at numerical scoring (eg 1-5) \n",
    " - The order of canditate responses matter\n",
    " - Foundational models tend to prefer their own outputs over other models\n",
    " - LLMs prefer longer respones and \"style\" over accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dee179152447b49be25b343342fb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running eval using claude-3-haiku-20240307:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-haiku-20240307 Score: 33.33333333333333%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a8dda74af74e598048d6bac207a5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running eval using claude-3-opus-20240229:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-opus-20240229 Score: 33.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "#TODO - Alex - align this eval with the above examples\n",
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(answer, rubric):\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, \n",
    "    and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. \n",
    "    Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect \n",
    "    inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "# Now we define the full grade_completion function.\n",
    "import re\n",
    "\n",
    "def grade_completion(output, golden_answer, model_name=FAST_MODEL_NAME):\n",
    "    messages = build_grader_prompt(output, golden_answer)\n",
    "    completion = get_completion(messages, model_name=model_name)\n",
    "    # Extract just the label from the completion (we don't care about the thinking)\n",
    "    pattern = r'<correctness>(.*?)</correctness>'\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "\n",
    "grades = []\n",
    "for output, question in tqdm(zip(outputs, eval), total=len(eval), desc=f'Running eval using {FAST_MODEL_NAME}'):\n",
    "    grade = grade_completion(output, question['golden_answer'], model_name=FAST_MODEL_NAME)\n",
    "    grades.append(grade)\n",
    "\n",
    "print(f\"{FAST_MODEL_NAME} Score: {grades.count('correct')/len(grades)*100}%\") \n",
    "\n",
    "# Run the grader function on our outputs and print the score using the smart model\n",
    "grades = []\n",
    "for output, question in tqdm(zip(outputs, eval), total=len(eval), desc=f'Running eval using {SMART_MODEL_NAME}'):\n",
    "    grade = grade_completion(output, question['golden_answer'], model_name=SMART_MODEL_NAME)\n",
    "    grades.append(grade)\n",
    "\n",
    "print(f\"{SMART_MODEL_NAME} Score: {grades.count('correct')/len(grades)*100}%\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.3 HITL - Human in the loop evaluation grading\n",
    "\n",
    "--- iteration on evals to improve them ---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
