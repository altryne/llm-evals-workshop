{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs in production - Evals Workshop - by Weights & Biases\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altryne/llm-evals-workshop/blob/main/eval.ipynb) [![Weights & Biases](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.me/weave-workshop-jan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Intro\n",
    "This notebook is accompanying a workshop, that will walk you through common patterns in building evaluations for LLMs, and useful rules of thumb to follow when doing so using [W&B Weave](https://wandb.me/weave-workshop-jan)\n",
    "\n",
    "We'll explore the following methodology for productizing robust LLM applications: \n",
    "\n",
    "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
    "\n",
    "\n",
    "Make sure to set your WANDB_API_KEY (get your key from [here](https://wandb.ai/authorize)) and OPENROUTER_API_KEY (or OPENAI_API_KEY if you have that) in the environment variables.\n",
    "\n",
    "If you're running in Colab, set the variables in the keys section on the left. \n",
    "\n",
    "If you want to self explore, find the `#TODO:` comments and replace them with your own code, then run the cell.\n",
    "\n",
    "Prepared by [Alex Volkov](https://twitter.com/altryne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages\n",
    "try:\n",
    "    import google.colab\n",
    "    !git clone -q --branch main https://github.com/altryne/llm-evals-workshop\n",
    "    %cd llm-evals-workshop\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print('⏳ Installing packages')\n",
    "%pip install -q uv\n",
    "!uv pip install -q --system weave gradio set-env-colab-kaggle-dotenv tqdm ipywidgets requests openai pillow\n",
    "print('✅ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext gradio\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import requests \n",
    "import io\n",
    "from set_env import set_env\n",
    "import json\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import weave\n",
    "from weave.flow.annotation_spec import AnnotationSpec\n",
    "\n",
    "load_dotenv()\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# initialize weave\n",
    "weave_api = weave.init('jan-evals-workshop')\n",
    "\n",
    "# initialize annotations for this project\n",
    "annotation = weave.publish(AnnotationSpec(\n",
    "    name=\"Doomer or Boomer\",\n",
    "    description=\"Doomer or Boomer or Neither\",\n",
    "    field_schema={ \"type\": \"string\", \"enum\": [\"Doomer\", \"Boomer\", \"Neither\"],},\n",
    "), \"doomer_or_boomer\")\n",
    "\n",
    "annotation_reason = weave.publish(AnnotationSpec(\n",
    "    name=\"Reason\",\n",
    "    description=\"Reason why you chose this value, write before clicking.\",\n",
    "    field_schema={ \"type\": \"string\"},\n",
    "), \"reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our LLM client, we'll use either Gemini or OpenAI\n",
    "API_PROVIDER = 'OpenRouter' # @param [\"Gemini\", \"OpenAI\", \"OpenRouter\"]\n",
    "if API_PROVIDER == 'Gemini':\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "    )\n",
    "    model = \"gemini-2.0-flash-exp\"\n",
    "elif API_PROVIDER == 'OpenRouter':\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    )\n",
    "    model = \"openai/chatgpt-4o-latest\"\n",
    "    # model = \"google/gemini-flash-1.5-exp\"\n",
    "    # model = \"deepseek/deepseek-chat\"\n",
    "else:\n",
    "    client = OpenAI()\n",
    "    model = \"chatgpt-4o-lates\"\n",
    "\n",
    "# Load the Jinja2 environment\n",
    "env = Environment(loader=FileSystemLoader('templates'))\n",
    "template = env.get_template('post.html.jinja')\n",
    "\n",
    "# Load replies data\n",
    "def load_replies():\n",
    "    replies = []\n",
    "    # Load replies from both files\n",
    "    with open('data/replies_alpin.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    with open('data/replies_daniel.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    return replies\n",
    "\n",
    "\n",
    "def get_random_post_and_analyze():\n",
    "    replies = load_replies()\n",
    "    post = random.choice(replies)\n",
    "    \n",
    "    # Format the post data for the template\n",
    "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
    "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
    "    \n",
    "    # Convert AT URI to bsky.app URL\n",
    "    at_uri = post['post']['uri']\n",
    "    _, _, author_did, _, post_id = at_uri.split('/')\n",
    "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
    "    \n",
    "    # Analyze the post\n",
    "    #download the avatar and convert to PIL image\n",
    "    avatar_uri = post['post']['author']['avatar']\n",
    "    avatar_response = requests.get(avatar_uri)\n",
    "    avatar_pil = Image.open(io.BytesIO(avatar_response.content))\n",
    "\n",
    "    response_dict = analyze_post_sentiment(avatar_pil, post['post']['author']['displayName'], post['post']['record']['text'])\n",
    "    analysis = response_dict['response'].choices[0].message.content\n",
    "    weave_call_id = response_dict['weave_call_id']\n",
    "    \n",
    "    post_data = {\n",
    "        'author': post['post']['author'],\n",
    "        'created_at': formatted_date,\n",
    "        'text': post['post']['record']['text'],\n",
    "        'like_count': post['post'].get('likeCount', 0),\n",
    "        'repost_count': post['post'].get('repostCount', 0),\n",
    "        'has_image': False,\n",
    "        'post_url': post_url\n",
    "    }\n",
    "    \n",
    "    return template.render(**post_data), analysis, weave_call_id, ''\n",
    "\n",
    "\n",
    "def submit_feedback(user_selection, reason, weave_call_id):\n",
    "    \"\"\"\n",
    "    Example function that could send user feedback (the user_selection)\n",
    "    and the weave_call_id to your Weave (or any other) API.\n",
    "    \"\"\"\n",
    "    call = weave_api.get_call(weave_call_id)\n",
    "\n",
    "    if reason:\n",
    "        print(\"reason\", reason)\n",
    "        reason_resp = weave_api.server.feedback_create(\n",
    "            {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.reason\",\n",
    "            \"annotation_ref\": annotation_reason.uri(),\n",
    "            \"payload\": {\"value\": reason},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    resp = weave_api.server.feedback_create(\n",
    "        {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.doomer_or_boomer\",\n",
    "            \"annotation_ref\": annotation.uri(),\n",
    "            \"payload\": {\"value\": user_selection},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Ready to analyze the next post\n",
    "    return get_random_post_and_analyze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tracing LLM calls with Weave\n",
    "\n",
    "#### Why Tracing is Important for LLM Application Reliability\n",
    "\n",
    "In building reliable LLM-based applications, having a clear view into\n",
    "how your system behaves is crucial. That’s where “tracing” comes in.\n",
    "\n",
    "1. **Detailed Interaction Records**:\n",
    "   Tracing captures all the inputs, prompts, responses, and any user feedback.\n",
    "   By preserving this detailed record, you always have the context needed to\n",
    "   debug unexpected or incorrect results.\n",
    "\n",
    "2. **Rapid Issue Diagnosis**:\n",
    "   With thorough traces, you can pinpoint issues faster—often without\n",
    "   needing direct access to remote systems. Simply reviewing the logs can\n",
    "   reveal how a certain response was triggered.\n",
    "\n",
    "3. **Collaboration and Sharing**:\n",
    "   Traces can be shared with both technical and non-technical stakeholders.\n",
    "   This not only streamlines collaboration but also ensures everyone is\n",
    "   working off the same “source of truth” when investigating bugs\n",
    "   or brainstorming improvements.\n",
    "\n",
    "4. **Outlier Spotting and Performance Tuning**:\n",
    "   By tracking calls at scale, you can detect when responses deviate\n",
    "   dramatically from the norm, troubleshoot any failures, and identify\n",
    "   potential performance bottlenecks.\n",
    "\n",
    "5. **Facilitates Product Evolution**:\n",
    "   As you enhance or expand your LLM application, comprehensive\n",
    "   tracing data helps you make more informed decisions about what to\n",
    "   improve, remove, or refine.\n",
    "\n",
    "With W&B Weave, comprehensive tracing is just 1 line of code, and offers features such as:\n",
    "- Syntax highlighting specific to your use-case (Markdown, JSON, etc.)\n",
    "- Ability to share links with other members of your team\n",
    "- Ability to filter traces by function name, input, output, etc.\n",
    "- Tracking latency, token count and cost per call (and trends)\n",
    "- Code associated with the llm call and versioning\n",
    "- Ability to add metadata per trace\n",
    "\n",
    "If you need to instrument existing code, you can use the `@weave.op` decorator to trace the function.  \n",
    "\n",
    "![CleanShot 2024-04-08 at 14 15 40@2x](https://gist.github.com/assets/463317/4e9ada49-572f-47d9-91e1-55ab72b2a476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add tracing to this function - then see how this function is traced in the Weave UI\n",
    "\n",
    "def analyze_post_sentiment(avatar, displayName, text):\n",
    "    # Prompt for OpenAI to analyze the sentiment\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither]. \n",
    "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    \\n\\n {displayName}: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Add some more context to the prompt\n",
    "    # prompt = f\"\"\"Analyze the following Bluesky post and determine if the author is a:\n",
    "    # - DOOMER (someone who hates AI and uses derogatory language)\n",
    "    # - BOOMER (someone who doesn't understand AI and asks to remove their data)\n",
    "    # - NEITHER (neutral or positive response)\n",
    "    \n",
    "    # Post: {displayName}: \"{text}\"\n",
    "    \n",
    "    # Respond with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    # \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        current_call = weave.require_current_call()\n",
    "        weave_call_id = current_call.id\n",
    "    except:\n",
    "        weave_call_id = None\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"weave_call_id\": weave_call_id\n",
    "    }\n",
    "\n",
    "# Lets test this out without tracing first\n",
    "response_dict = analyze_post_sentiment(\"\",\"Alex\",\"I hate AI\")\n",
    "\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even without @weave.op, since Weave is initialized, it will still trace the function call and store it in the Weave project as it automatically understands that we use OpenAi client. However if we add @weave.op, we can get even more detail and insrument our existing code with Weave.\n",
    "\n",
    "Tracing becomes even more useful when you have a lot of nested calls, such as a multi-step chat conversation, or a RAG system with retrieval, or an agentic system with multiple steps.\n",
    "\n",
    "![text](https://cln.sh/Sc8ZtrdM+)\n",
    "\n",
    "Here's a great comlext example of our agent Winston [text](https://wandb.ai/wandb-designers/winston/weave/traces?cols=%7B%22attributes.weave.client_version%22%3Afalse%2C%22attributes.weave.os_name%22%3Afalse%2C%22attributes.weave.os_release%22%3Afalse%2C%22attributes.weave.os_version%22%3Afalse%2C%22attributes.weave.source%22%3Afalse%2C%22attributes.weave.sys_version%22%3Afalse%7D&peekPath=%2Fwandb-designers%2Fwinston%2Fcalls%2F0193ff3f-54d7-73a3-8004-0a582a594307%3Fpath%3Dwinston-solve*0%2Bvincent-execute*0%26tracetree%3D1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. User Feedback & Annotations\n",
    "\n",
    "Collecting user feedback is a crucial way to improve your LLM applications. There's a reason that every chatbot you use has 👍/👎 and a text box to leave feedback. This is one of the best ways for those labs to understand and improve their models and align them to user preferences.\n",
    "\n",
    "![text](https://cln.sh/JGMBxMtH+)\n",
    "\n",
    "Users don't have to be external as well, as you develop your application, marking traces as \"good\" or \"bad\", and adding why, is a great way to kick start your initial evaluation dataset with working and non-working examples. \n",
    "\n",
    "Additionally, after logging hundreds of thousads of traces, they will all start looking the same, so additional context like your user's feedback, will greately improve your ability to look at your data and find the outliers.\n",
    "\n",
    "Weave supports collecting user Feedback in the UI and also via the API so you can collect it from your users and also leave it yourself while looking at your data. \n",
    "\n",
    "![text](https://cln.sh/X6fFHD8t+)\n",
    "\n",
    "Read more about feedback [here](https://weave-docs.wandb.ai/guides/tracking/feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2.1 Doomer or Boomer App - Annotations by example\n",
    "\n",
    "Unlike user feedback, Annotations are a bit of a more structure way to classify responses, to help create a dataset of golden answers and reasons or rationales for those answers. All of the major companies use Scale.ai for this and pay them a LOT of money, but you don't have to right away, you can start small, by yourself or with your team. \n",
    "\n",
    "Let's see how we can kickstart a simple dataset of annotations by a practical example.\n",
    "\n",
    "![image](https://gist.github.com/user-attachments/assets/a8537545-e070-4c8e-9988-2a8a905b9d2c)\n",
    "\n",
    "To simulate a real world scenario, we'll build a simple app that will allow you to annotate a few posts. \n",
    "\n",
    "In our case, we're pretending to work at a company that's trying to build an AI classifier for Bluesky posts. We're humans that work in the company and are helping it to align and finetune models for AI moderation. \n",
    "\n",
    "We've compiled replies from BlueSky users, on 2 posts that collected publicly available data from BlieSky to train AI models (BlueSky data is public), which led to a lot of hate by users on BlueSky. \n",
    "\n",
    "We're going to build a simple app that will use an LLM to classify the replies into 3 categories: `Doomer`, `Boomer`, or `Neither`. \n",
    "\n",
    "`Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "`Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "`Neither`: Folks who reply neutral or positive to the post.\n",
    "\n",
    "At first our LLMs will not have context to the task, so won't be able to reliably classify the replies, so a human is needed to annotate with additional context, you are that human. \n",
    "\n",
    "Launch the app and go through a few posts, annotate with a reason for your choice and the correct classification, we'll later use this data to align/finetune our LLM to classify the replies more accuretly and reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%blocks\n",
    "# Create a Gradio Blocks app\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'false'\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🦋 Doomer or Boomer\n",
    "    Our AI analyzes bluesky replies and posts to determine if the author is a doomer or a boomer.  \n",
    "    Source of data: Replies to a post by a BlueSky user that compiled a dataset of posts, which went viral and generated a lot of hate on BlueSky.  \n",
    "    These are replies and comments on 2 posts that collected a dataset of posts of BlueSky users to train AI models (BlueSky data is public)\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            post_html = gr.HTML()\n",
    "            next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "            #### Instructions for labeler: \n",
    "            `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "            `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "            `Neither`: Folks who reply neutral or positive to the post.\n",
    "            \n",
    "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"Analysis Results\",\n",
    "                placeholder=\"Analysis will appear here...\",\n",
    "                lines=4\n",
    "            )\n",
    "            weave_call_id_state = gr.State()\n",
    "            \n",
    "            # Replace dropdown with three buttons\n",
    "            reason_input = gr.Textbox(label=\"Add reason and click\",placeholder=\"Reason why you chose this value, write before clicking.\", lines=2)\n",
    "            with gr.Row():\n",
    "                doomer_btn = gr.Button(\"Doomer 😡\", variant=\"huggingface\")\n",
    "                boomer_btn = gr.Button(\"Boomer 👵\", variant=\"primary\")\n",
    "                neither_btn = gr.Button(\"Neither 🤷\")\n",
    "\n",
    "            \n",
    "    # Set up event handler for combined next/analyze\n",
    "    next_post_btn.click(fn=get_random_post_and_analyze, outputs=[post_html, analysis_output, weave_call_id_state, reason_input])\n",
    "    \n",
    "    doomer_btn.click(\n",
    "    fn=submit_feedback,\n",
    "    inputs=[gr.State(\"Doomer\"), reason_input, weave_call_id_state],\n",
    "    outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    boomer_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Boomer\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    neither_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Neither\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Initialize with first post and analysis\n",
    "    post_html.value, analysis_output.value, weave_call_id_state.value, reason_input.value = get_random_post_and_analyze()\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Building a dataset from annotated calls\n",
    "\n",
    "Now that we've annotated at least 10-20 examples, we can build our first evaluation dataset! \n",
    "\n",
    "![text](https://cln.sh/dyBq4QXD+)\n",
    "\n",
    "Step 1: Filter calls in Weave UI by only those with annotations not empty\n",
    "\n",
    "Step 2: Use the Export -> Use Python button to get code to extract a list of filtered annotated calls\n",
    "\n",
    "Step 3: Convert the calls to a clean evaluation dataset (and optionally publish to Weave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@weave.op\n",
    "def get_annotated_calls():\n",
    "   # Weave API call to get all calls filtered by annotations not empty (with reasons)\n",
    "   resp = weave_api.server.calls_query_stream({\n",
    "      \"project_id\": weave_api._project_id(),\n",
    "      \"filter\": {\"op_names\": [f\"weave:///{weave_api._project_id()}/op/analyze_post_sentiment:*\"]},\n",
    "      \"query\": {\"$expr\":{\"$and\":[{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.doomer_or_boomer].payload.value\"},{\"$literal\":\"\"}]}]},{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.reason].payload.value\"},{\"$literal\":\"\"}]}]}]}},\n",
    "      \"sort_by\": [{\"field\":\"started_at\",\"direction\":\"desc\"}],\n",
    "      \"include_feedback\": True,\n",
    "   })\n",
    "\n",
    "   # Iterate over the calls, clean up and publish as a dataset we can version and reference later.\n",
    "   list_of_calls = []\n",
    "   dataset = []\n",
    "   for call in resp:\n",
    "      try:\n",
    "         row = {}\n",
    "         call_dict = dict(call)\n",
    "         row[\"input\"] = call_dict.get('inputs').get('text')\n",
    "         row[\"displayName\"] = call_dict.get('inputs').get('displayName')\n",
    "         row[\"llm_classification\"] = call_dict.get('output')[0]\n",
    "         list_of_feedback = call_dict.get('summary').get('weave').get('feedback')\n",
    "         for feedback in list_of_feedback:\n",
    "            if feedback.get(\"feedback_type\") == 'wandb.annotation.doomer_or_boomer':\n",
    "               row[\"human_annotation\"] = feedback.get('payload').get('value')\n",
    "            if feedback.get(\"feedback_type\") == 'wandb.annotation.reason':\n",
    "               row[\"reason\"] = feedback.get('payload').get('value')\n",
    "      except Exception as e:\n",
    "        continue\n",
    "      \n",
    "      dataset.append(row)\n",
    "\n",
    "   weave.publish(weave.Dataset(name=\"doomer_or_boomer_dataset\", rows=dataset))\n",
    "   return dataset\n",
    "\n",
    "dataset = get_annotated_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Storing Datasets within Weave\n",
    "\n",
    "If you'd like to store your own dataset and name them, it's very easy to do so, and then you get a \"ref\" to the dataset that's stored in our system. Weave datasets are versioned, which means you can reference them in your code by a URL or a ref, and either point to the latest version or a specific version. \n",
    "\n",
    "Using `refs` is a great way to make your code reproducible and versioned.\n",
    "\n",
    "![CleanShot 2025-01-07 at 16 12 35@2x](https://gist.github.com/user-attachments/assets/e2d02340-cc0f-41e8-8d97-957b08611d08)\n",
    "\n",
    "\n",
    "Here's an example of the dataset we just created, and how we can reuse it in our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace this dataset with your own using the dataset link above and looking at the \"use\" tab\n",
    "doomer_or_boomer_dataset = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset\").get()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(doomer_or_boomer_dataset.rows)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Evaluations \n",
    "### Components of an Evaluation\n",
    "\n",
    "Evaluations generally consist of four key elements:\n",
    "- An **input prompt** that serves as the basis for the model's completion. This prompt often includes a set of variable inputs that are inserted into a prompt template during testing.\n",
    "- The **output** generated by the model in response to the input prompt.\n",
    "- A **\"gold standard\" answer** used as a reference for assessing the model's output. This can be an exact match that the output must replicate, or an exemplary answer that provides a benchmark for scoring.\n",
    "- A **score**, determined by one of the scoring approaches outlined below, which indicates the model's performance on the question.\n",
    "\n",
    "#TODO: Look at the dataset and try to match the input, output, gold standard each row\n",
    "\n",
    "## Evaluation Grading Approaches\n",
    "Evaluations can be time-consuming and costly in two main areas: creating questions and gold standard answers, and the scoring/grading process itself.  \n",
    "Developing questions and ideal answers is often a one-time fixed cost, albeit potentially time-intensive if a suitable dataset is not readily available (consider leveraging an LLM to generate questions!). However, scoring is a recurring expense incurred each time the evaluation is conducted, which is likely to be frequent. Therefore, designing evaluations that can be scored efficiently and economically should be a central priority.\n",
    "\n",
    "![](https://gist.github.com/assets/463317/e970bb03-9552-4712-ba12-727b89928e3b)\n",
    "\n",
    "There are three primary methods for grading (scoring) evaluations:  \n",
    "- **Programmatic:** This approach involves using standard code (primarily string matching and regular expressions) to assess the model's outputs. Common techniques include checking for an exact match against an answer or verifying the presence of key phrase(s) in a string. Programmatic scoring is the most optimal method when feasible, as it is extremely fast and highly reliable. However, not all evaluations are amenable to this style of scoring. \n",
    "  - Goes great with structured output - validate against an enum\n",
    "  - Code generation output - does it run, is valid, does it compile? \n",
    "  - Tool use validation - do the tools exist? \n",
    "- **Human in the loop:** In this approach, a human reviewer examines the model-generated answer, compares it to the gold standard, and assigns a score. While manual scoring is the most versatile method, applicable to nearly any task, it is also exceptionally slow and costly, especially for large-scale evaluations. Designing evaluations that necessitate manual scoring should be avoided whenever possible.\n",
    "  - Domain specific & expert information\n",
    "  - Sensitive topics\n",
    "- **Model-based scoring AKA LLM as a judge:** LLMs (especially Claude, GPT-4o, Gemini) are really good at grading themselves (or even outputs of other LLMs) especially in wide range of tasks that traditionally needed human judgement like tone in creative writing or accuracy in open-ended question, or classification. This model-based scoring is accomplished by creating a _scorer prompt_ for an LLM\n",
    "  - Open ended style questions\n",
    "  - Classification & Translation \n",
    "  - Instruction following\n",
    "\n",
    "Let's explore an example of each\n",
    "\n",
    "## 3.1 Programmatic scoring \n",
    "\n",
    "Here we have a simple programmatic eval that will try and check if the LLM had the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for alex - this example for programmatic needs more Juice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a programmatic scorer that will compare the ground truth to the LLM answer and check if it is correct\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "def programmatic_scorer(output: str, human_annotation: str):\n",
    "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "    # we expect this evaluation to fail becuase the LLM is talking alot and never returns just the reason\n",
    "    if not output or not human_annotation:\n",
    "        raise ValueError(\"Model output or human annotation is empty\")\n",
    "    return {\"match\": output == human_annotation}\n",
    "\n",
    "# TODO: change the programmatic scorer (commented below) to check if the output includes the reason string (Doomer, Boomer, Neither)\n",
    "# check for lower case and upper case, and check if more than one of the options is present, meaning that LLM wasn't sure\n",
    "# add the programmatic scorer to the evaluation\n",
    "\n",
    "# def programmatic_scorer(output: str, human_annotation: str):\n",
    "#     # check if model_output includes the human_annotation only once \n",
    "#     if human_annotation.lower() in output.lower():\n",
    "#         #possible match, now lets check if the model_output includes any of the other options but not the human_annotation\n",
    "#         for option in [\"doomer\", \"boomer\", \"neither\"]:\n",
    "#             if option.lower() in output.lower() and option.lower() != human_annotation.lower():\n",
    "#                 return {\"match\": False}\n",
    "#         return {\"match\": True}\n",
    "#     return {\"match\": False}\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset, scorers=[programmatic_scorer]\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(input: str):\n",
    "    # here's where you would add your LLM call and return the output\n",
    "    # since we already called the LLM, we can just iterate over the dataset \n",
    "    # and return the llm_classification where the question is the same\n",
    "    row = [row for row in doomer_or_boomer_dataset.rows if row['input'] == input]\n",
    "    return row[0].get('llm_classification')\n",
    "\n",
    "await evaluation.evaluate(function_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strucutred outputs with programmatic scorers\n",
    "\n",
    "The above example likely gave us a score of 0, because LLMs like to talk, and comparing that via a simple string match is not going to work. \n",
    "\n",
    "Programmatic scorers work great when we have structured outputs and we know exactly what to expect from LLMs. Let's recreate our LLM calls for the same questions with strucutred outputs so we can compare the LLM output directly to the human annotation and see if we can get a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WEAVE_PARALLELISM'] = '5'\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "\n",
    "@weave.op()\n",
    "def with_structured_llm_call(input: str, displayName: str):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither]. \n",
    "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "\n",
    "    \n",
    "    Text to Classify: \n",
    "    \\n\\n {displayName}: \"{input}\"\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO: add a request for structured output in JSON format\n",
    "    # prompt += \"\"\"\n",
    "    # Respond in JSON format with this exact schema   {{\n",
    "    #     \"classification\": \"DOOMER\" | \"BOOMER\" | \"NEITHER\",\n",
    "    #     \"reason\": \"string\"\n",
    "    # }}\n",
    "    # \"\"\"\n",
    "\n",
    "    ## TODO: request a stricter JSON \n",
    "    # prompt += \"\"\"\n",
    "    #     with no backticks or quotes or anything else, just valid JSON or I lose my job  \n",
    "    # \"\"\"\n",
    "\n",
    "    ## TODO: With additional context about the classification criteria (copy from above)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def programmatic_scorer(output: str, human_annotation: str):\n",
    "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "    if not output:\n",
    "        raise ValueError(\"Model output is empty\")\n",
    "    try:\n",
    "        object = json.loads(output)\n",
    "    except:\n",
    "        raise ValueError(\"Model output is not valid JSON\")\n",
    "    \n",
    "    return {\"match\": object.get('classification').lower() == human_annotation.lower()}\n",
    "\n",
    "new_evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset, scorers=[programmatic_scorer]\n",
    ")\n",
    "\n",
    "await new_evaluation.evaluate(with_structured_llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 HITL - Human in the loop evaluation grading\n",
    "\n",
    "Programmatic scoring is great for many reasons, cheap to get started with, can run very fast and can be very reliable, but cannot cover open ended questions or tasks that require analysis or judgement. \n",
    "\n",
    "For example, did the LLM follow the instructions it was given, did it hallucinate, was it verbose or concise, etc.\n",
    "\n",
    "To judge those outputs we can use human graders, to provide \"golden answers\", which is what we did above with the annotation example with our Doomer or Boomer app. \n",
    "\n",
    "The downside of HITL is that it's slow, expensive, and not scalable (unless you have a lot of money in he bank). \n",
    "\n",
    "HITL is a great way to kickstart an evaluation dataset and extarpolate with an LLM. \n",
    "\n",
    "Here's a slight alternative on our app, that shows LLM responses and allows our humans in the loop to judge the responses as correct or incorrect. \n",
    "\n",
    "Run this app, mark up to 10 responses, and then hit evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Evaluation\n",
    "dataset_of_doomer_or_boomer = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset_with_structured_output:EwbD2kvMzz1R8nY6IxY6EALPj7XV6XYue44gQWDgDKE\").get()\n",
    "\n",
    "def match_dataset_with_replies():\n",
    "    matched_replies = []\n",
    "    for row in dataset_of_doomer_or_boomer.rows:\n",
    "        # Find matching reply in all_replies\n",
    "        for reply in load_replies():\n",
    "            if reply['post']['record']['text'] == row['input']:\n",
    "                matched_reply = {\n",
    "                    'full_reply': reply,\n",
    "                    'input': row.get('input', ''),\n",
    "                    'output': row.get('output', ''),\n",
    "                    'reason': row.get('reason', ''),\n",
    "                    'llm_classification': row.get('llm_classification', ''),\n",
    "                    'displayName': row.get('displayName', '')\n",
    "                }\n",
    "                matched_replies.append(matched_reply)\n",
    "                break\n",
    "    return matched_replies\n",
    "\n",
    "matched_replies = match_dataset_with_replies()\n",
    "annotated_rows = []\n",
    "\n",
    "def get_next_annotated_post(current_index:int = 0):\n",
    "    # Get the matched replies\n",
    "    \n",
    "    print(current_index, len(matched_replies))\n",
    "    if current_index >= len(matched_replies):\n",
    "        current_index = 0  # Reset to beginning if we've reached the end\n",
    "        \n",
    "    reply = matched_replies[current_index]\n",
    "    post = reply['full_reply']\n",
    "    \n",
    "    # Format the post data for the template\n",
    "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
    "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
    "    \n",
    "    # Convert AT URI to bsky.app URL\n",
    "    at_uri = post['post']['uri']\n",
    "    _, _, author_did, _, post_id = at_uri.split('/')\n",
    "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
    "    \n",
    "    post_data = {\n",
    "        'author': post['post']['author'],\n",
    "        'created_at': formatted_date,\n",
    "        'text': post['post']['record']['text'],\n",
    "        'like_count': post['post'].get('likeCount', 0),\n",
    "        'repost_count': post['post'].get('repostCount', 0),\n",
    "        'has_image': False,\n",
    "        'post_url': post_url\n",
    "    }\n",
    "    \n",
    "    # Use the stored LLM classification and human annotation\n",
    "    analysis = f\"\"\"LLM Classification: {reply['llm_classification']}\n",
    "    \n",
    "LLM Reasoning: {reply['reason']}\n",
    "    \"\"\"\n",
    "    \n",
    "    run_evaluation_btn = {\n",
    "        \"interactive\": True if len(annotated_rows) >=  10 else False,\n",
    "        \"value\": \"Run Evaluation\" if len(annotated_rows) >=  10 else f\"Annotate {10 - len(annotated_rows)} more posts\"\n",
    "    }\n",
    "    return template.render(**post_data), analysis, current_index + 1, gr.update(**run_evaluation_btn), \"\"\n",
    "\n",
    "def submit_hitl_feedback(correct_or_incorrect: str, feedback: str, next_index: int):\n",
    "    annotated_rows.append({\n",
    "        \"input\": matched_replies[next_index-1].get('input'),\n",
    "        \"output\": matched_replies[next_index-1].get('output'),\n",
    "        \"llm_classification\": matched_replies[next_index-1].get('llm_classification'),\n",
    "        \"correct_or_incorrect\": True if correct_or_incorrect == \"correct\" else False,\n",
    "        \"human_reason_for_correct_or_incorrect\": feedback,\n",
    "    })\n",
    "    return get_next_annotated_post(next_index)\n",
    "\n",
    "\n",
    "def right_according_to_human(output: str, correct_or_incorrect: bool):\n",
    "    return correct_or_incorrect\n",
    "\n",
    "@weave.op()\n",
    "def return_input_row(input: str):\n",
    "    return [x for x in annotated_rows if x.get('input') == input]\n",
    "\n",
    "async def run_evaluation():\n",
    "    hitl_evaluation = Evaluation(\n",
    "        dataset=annotated_rows,\n",
    "        scorers=[right_according_to_human],\n",
    "        name=\"hitl_evaluation\"\n",
    "    )\n",
    "    \n",
    "    result = await hitl_evaluation.evaluate(return_input_row)\n",
    "    gr.Info('Evaluation complete! Check your Weave project for the results.')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%blocks\n",
    "# Create a Gradio Blocks app\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as new_demo:\n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Human in the loop\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(f\"\"\"## 1. Post to Analyze  \"\"\")\n",
    "            post_html = gr.HTML()\n",
    "            # next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "            #### Instructions for HTIL judge: \n",
    "            - review LLM outputs and mark them as correct or incorrect  \n",
    "            - after 10-20 examples, hit \"run evaluation\" button\n",
    "    \n",
    "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
    "            \"\"\")\n",
    "            \n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            \n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"2. Review LLM Classification for this post\",\n",
    "                placeholder=\"Analysis will appear here...\",\n",
    "                lines=4,\n",
    "            )\n",
    "            next_index = gr.State(value=0)\n",
    "            \n",
    "            with gr.Accordion(\"Reminder of Doomer, Boomer, or Neither Criteria\", open=False):\n",
    "                gr.Markdown(f\"\"\"\n",
    "                `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "                `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "                `Neither`: Folks who reply neutral or positive to the post.\n",
    "                \"\"\")\n",
    "            # Replace dropdown with three buttons\n",
    "            reason_input = gr.Textbox(label=\"3. Add reason and submit\",placeholder=\"Reason why the LLM got this classification right or wrong\", lines=2)\n",
    "            with gr.Row():\n",
    "                correct_btn = gr.Button(\"LLM is Correct 👍\")\n",
    "                incorrect_btn = gr.Button(\"LLM is Incorrect 👎\")\n",
    "\n",
    "            run_evaluation_btn = gr.Button(\"Run Evaluation\", variant=\"primary\", interactive=False)\n",
    "\n",
    "            \n",
    "    # Set up event handler for combined next/analyze\n",
    "    # next_post_btn.click(fn=get_next_annotated_post, inputs=[next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
    "    \n",
    "    correct_btn.click(fn=submit_hitl_feedback, inputs=[gr.State(\"correct\"), reason_input, next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
    "    incorrect_btn.click(fn=submit_hitl_feedback, inputs=[gr.State(\"incorrect\"), reason_input, next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
    "\n",
    "    run_evaluation_btn.click(fn=run_evaluation, inputs=[], outputs=[analysis_output])\n",
    "    # Initialize with first post and analysis\n",
    "    post_html.value, analysis_output.value, next_index.value, run_evaluation_btn.value, reason_input.value = get_next_annotated_post()\n",
    "\n",
    "new_demo.queue()\n",
    "new_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 LLM as a Judge - use another LLM to grade your LLM outputs\n",
    "\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! \n",
    "\n",
    "We can actually have an LLM do the grading for us. We'll use a teacher model to grade the LLM outputs of a \"student\" model (in this case the LLM we're using for our production system is the student). \n",
    "\n",
    "There are a few issues with this approaches to be aware of: \n",
    " - LLMs are not great at numerical scoring (eg 1-5) \n",
    " - The order of canditate responses matter\n",
    " - Foundational models tend to prefer their own outputs over other models\n",
    " - LLMs prefer longer respones and \"style\" over accuracy\n",
    "\n",
    "\n",
    "## Let's build our LLM judge\n",
    "\n",
    "First, we'll start by building a \"grader prompt\" template, a prompt asking our judge to perform the judging itself. This will be our iteration grounds. In this template, we'll inject both the output of our production LLM model, and the criteria / rules or rubric that makes an answer correct or incorrect. \n",
    "\n",
    "In our case, the classification into one of 3 (Doomer, Boomer, Neither) is done \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it reflects frustration and criticism rather than a characteristic worldview of Doomers or Boomers. However, based on the criteria, the comment uses derogatory language ('You're a fucking idiot') and expresses hostility towards the author of the post, which aligns with the definition of a 'Doomer' as someone who hates AI and uses derogatory language. Therefore, the automated assistant's classification is incorrect, and the correct classification is 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it expresses frustration with data collection rather than generational pessimism or outdated views. However, based on the criteria, the use of derogatory language ('shitty dataset' and 'take me the fuck out') indicates hostility towards the AI effort and the authors of the post. This aligns with the definition of a 'doomer,' as the comment reflects hate for AI and the dataset usage. Therefore, the automated assistant's reasoning is incorrect, and the correct classification is 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the post expresses a general positive sentiment about freedom of information and does not align with either doomer or boomer stereotypes. Based on the criteria, this classification is correct because the comment is neutral to positive and does not exhibit hate towards AI or misunderstanding of AI. It does not use derogatory language or copy-paste a removal request, so it fits the 'neither' category.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it deemed the post too short and lacking specific generational or optimistic/pessimistic context. However, based on the criteria, the use of derogatory language ('What the fuck is wrong with you?') directed at the author aligns with the definition of a 'doomer,' as it reflects hostility and negativity likely stemming from hate for AI or the dataset usage. The assistant's reasoning is incorrect because it overlooked the derogatory tone, which is a key indicator of a 'doomer' classification according to the criteria.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it deemed the post too short and lacking generational or pessimistic context. However, the term 'chode' is a derogatory term and, in this context, is likely being used as an insult towards the author of the post. According to the criteria, individuals who use derogatory language towards the author due to their hate for AI or the dataset are classified as 'doomer.' Therefore, the automated assistant's reasoning is incorrect, and the correct classification is 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because it expresses a desire for catastrophic technological failure, which aligns with the criteria for 'doomer' as someone who hates AI and uses derogatory language. The comment wishing for all devices and servers to be EMP'd is clearly hostile and reflects a negative attitude towards technology, fitting the 'doomer' classification. The reasoning provided by the assistant is valid and consistent with the criteria.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' which is correct. The comment is positive and supportive of the dataset's potential to improve LLM models. It does not exhibit any hate towards AI or the author (which would classify it as 'doomer'), nor does it show a lack of understanding or a copy-paste request to remove data (which would classify it as 'boomer'). Therefore, the reasoning and classification are valid based on the criteria provided.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it noted the post expressed concern about data privacy and included an insult, which it deemed did not fit into the Doomer or Boomer stereotypes. However, based on the criteria, the use of derogatory language ('you're a piece of shit') towards the author aligns with the definition of a Doomer, as it reflects hate and hostility towards AI and the dataset usage. Therefore, the automated assistant's reasoning is incorrect, and the correct classification is 'doomer'.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it uses internet culture humor and relatable anxiety rather than expressing hate for AI or misunderstanding it. Based on the criteria, the comment does not exhibit derogatory language towards AI or the author (doomer) nor does it copy-paste a request to remove data (boomer). The comment is neutral and humorous, aligning with the 'neither' classification. The assistant's reasoning is valid and aligns with the criteria provided.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it interpreted the post as aggressive but not indicative of a generational worldview. However, based on the criteria, the comment uses derogatory language ('shithead') and expresses hostility, which aligns with the definition of a 'doomer'—someone who hates AI and uses derogatory language towards the author of the post. Therefore, the automated assistant's reasoning is incorrect, and the correct classification is 'doomer'.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the post expresses disapproval but lacks specific generational markers. Based on the criteria, the comment does not use derogatory language towards the author or AI (which would classify it as 'doomer'), nor does it copy-paste a request to remove data (which would classify it as 'boomer'). The comment is a general expression of disapproval without any clear alignment to the 'doomer' or 'boomer' categories, so the classification of 'neither' is correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it was a humorous and absurdist observation. However, based on the criteria, the comment uses derogatory language ('Daniel van Shit') directed at the author of the post, which aligns with the behavior of a 'doomer' as described in the guidelines. The assistant's reasoning does not account for the derogatory nature of the comment, making its classification incorrect.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"boomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it expresses a concern about data privacy without aligning with either the doomer or boomer perspectives. However, based on the criteria, the comment appears to be a copy-paste style request for data removal, which aligns with the 'boomer' classification. The comment does not use derogatory language or express hate for AI, so it is not 'doomer.' The assistant's reasoning is incorrect because it overlooks the copy-paste nature of the request, which fits the 'boomer' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because the post is too short and lacks context to determine generational or ideological alignment. Based on the criteria, this reasoning is valid. The comment '@threv: how about not doing this' does not use derogatory language towards AI or the author, nor does it copy-paste a request to remove data. It is neutral and lacks sufficient context to classify it as 'doomer' or 'boomer.' Therefore, the automated assistant's classification is correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it is a straightforward request for data removal information without any generational or pessimistic viewpoint. Based on the criteria, this classification is correct because the comment does not exhibit derogatory language (which would indicate a 'doomer') or copy-pasting behavior (which would indicate a 'boomer'). The comment is neutral and polite, fitting the 'neither' category.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it is generally positive and encouraging. Based on the criteria, this classification is correct. The comment does not exhibit hate for AI or derogatory language (Doomer), nor does it involve a copy-pasted request to remove data (Boomer). Instead, it appears to be a neutral or positive statement encouraging personal growth, which aligns with the 'neither' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it deemed the post too short and lacking context to categorize. However, based on the criteria, the use of derogatory language ('Fuck off') directed at the handle suggests hostility and aligns with the behavior of a 'doomer,' who expresses hate and uses derogatory language towards the author due to their dislike of AI. The assistant's reasoning is incorrect because the comment's tone and language provide sufficient context to classify it as 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"boomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it interpreted the request as a neutral statement about data privacy. However, based on the criteria, the comment aligns with the 'boomer' classification. The user is requesting their data be removed, which matches the description of someone who doesn't understand AI and copy-pastes a request to remove their data. The assistant's reasoning is incorrect because it overlooked the specific behavior outlined in the criteria for 'boomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it interpreted the sentiment as nihilistic and angry, without aligning it with the specific characteristics of a 'boomer' or 'doomer.' However, based on the criteria, the use of derogatory language ('Fuck this and fuck you') directed at the author of the post aligns with the definition of a 'doomer,' as it reflects hate and hostility likely stemming from opposition to AI. The assistant's reasoning is incorrect because it overlooked the derogatory nature of the comment, which is a key indicator of a 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it deemed the post too short and lacking context to indicate a specific generational viewpoint. However, based on the criteria, the comment 'Delete your account' is derogatory and hostile towards the author, which aligns with the behavior of a 'doomer' as defined in the guidelines. The assistant's reasoning is incorrect because it failed to recognize the hostility in the comment, which is a key indicator of a 'doomer' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it was a neutral generational observation. However, based on the criteria, the comment appears to be hostile and sarcastic, implying a threat of legal action ('Can I sue you personally for this?'). This aligns with the behavior of a 'doomer,' as it reflects animosity towards the authors and their AI-related efforts. The assistant's reasoning is incorrect because it overlooks the hostile tone and intent of the comment, which fits the 'doomer' classification as per the guidelines provided.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"boomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it expresses concern about data privacy without aligning with either doomer or boomer viewpoints. However, based on the criteria, the comment fits the 'boomer' classification because it is a straightforward request to remove data, albeit with some typos, and does not use derogatory language or express hate towards AI. The user appears to misunderstand the context of AI and data scraping, which aligns with the 'boomer' classification. Therefore, the automated assistant's reasoning is incorrect.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it expresses frustration with data collection rather than a generational viewpoint. However, based on the criteria, the comment uses derogatory language ('shitty dataset') and threatens legal action, which aligns with the definition of a 'doomer'—someone who hates AI and uses derogatory language towards the author of the post due to their dislike of AI and data usage. Therefore, the automated assistant's reasoning is incorrect, and the correct classification is 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' due to its aggressive and derogatory tone, which aligns with the criteria provided. The comment uses insulting language ('little chickenshit') directed at the author, which reflects hostility and negativity towards the post. This matches the definition of a 'doomer' as someone who hates AI and uses derogatory language towards the author. Therefore, the automated assistant's reasoning and classification are correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it interpreted the language as absurdist humor rather than aligning with doomer or boomer tropes. However, based on the criteria, the comment uses derogatory language ('Choke down a dog’s piss, chum!') directed at the author of the post, which aligns with the behavior of a 'doomer' as described in the guidelines. The reasoning provided by the assistant is incorrect because it overlooks the hateful and derogatory nature of the comment, which is a key characteristic of a 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the post focuses on data privacy and GDPR compliance, which are not typically associated with Boomer or Doomer viewpoints. Based on the criteria, this classification is correct. The comment does not use derogatory language towards the author or AI (Doomer) nor does it copy-paste a request to remove data (Boomer). Instead, it raises a legitimate question about GDPR compliance in a neutral tone, which aligns with the 'neither' classification.\" \n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"boomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it is a neutral request for data removal and not indicative of a generational viewpoint. However, based on the criteria, the comment aligns with the 'boomer' classification because it is a straightforward request to remove data, which suggests a lack of understanding about AI and aligns with the behavior of copy-pasting such requests. The assistant's reasoning is incorrect as it overlooked the criteria for 'boomer.'\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m17\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m18\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m19\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m20\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m21\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m22\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m23\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m24\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m25\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m26\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m27\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'right_according_to_llm_judge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.37037037037037035</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.835951805114746</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'right_according_to_llm_judge'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m10\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.37037037037037035\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m15.835951805114746\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/thursdai/jan-evals-workshop/r/call/0194521b-8770-7363-afff-c8dd70fca559\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because the author uses derogatory language ('You're a fucking idiot') and expresses hostility toward the AI enthusiast. This aligns with the criteria for a 'doomer,' which includes hating AI and using derogatory language toward the author of the post. The reasoning provided by the assistant is valid and consistent with the criteria.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' based on the use of aggressive and derogatory language ('Take me the fuck out of your shitty dataset'). According to the criteria, a 'doomer' is someone who hates AI and uses derogatory language towards the author of the post because of their hate for AI and their data being used for AI. The comment clearly fits this description, as it expresses hostility and uses offensive language. Therefore, the automated assistant's reasoning and classification are correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' which is correct. The comment expresses gratitude and support for freedom of information, which aligns with the criteria for 'neither' as it is neutral or positive. There is no derogatory language or misunderstanding of AI present in the comment, so it does not fit the definitions of 'doomer' or 'boomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'Doomer' because it identified the use of derogatory language ('What the fuck is wrong with you?') as hostile towards the author. Based on the criteria, 'Doomer' is the correct classification for someone who uses derogatory language towards the author due to their hate for AI. The reasoning provided by the assistant aligns with the criteria, so the classification is valid.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because the term 'Chode.' is vague, lacks context, and does not directly express hate toward AI or the author, nor does it indicate a misunderstanding of AI or a copy-paste request. Based on the criteria, this reasoning is valid, as the comment does not meet the definitions of 'doomer' or 'boomer.' It is neutral and does not provide enough information to classify otherwise.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because the phrase 'May all your devices and servers get EMP'd' expresses hostility and derogatory intent. This aligns with the criteria for a 'doomer,' which includes using hateful or derogatory language towards the author due to their dislike of AI. The reasoning provided by the assistant is valid and consistent with the criteria.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' which aligns with the criteria provided. The comment is positive and supportive, expressing appreciation for the dataset's contribution to building better LLM models. There is no derogatory language or evidence of misunderstanding AI, nor is there a copy-pasted request to remove data. Therefore, the classification of 'neither' is correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because of the use of derogatory language ('you're a piece of shit') directed at the author of the post. According to the criteria, a 'doomer' is someone who hates AI and uses derogatory language towards the author due to their dislike of AI and their data being used for AI. The reasoning provided by the assistant aligns with the criteria, as the comment contains hostility and derogatory language, which fits the definition of a 'doomer.' Therefore, the classification is correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it expresses a general sentiment about privacy concerns without using derogatory language or copy-pasting a removal request. Based on the criteria, this classification is correct. The comment does not exhibit hostility or hate towards AI (which would classify it as 'doomer') nor does it copy-paste a removal request (which would classify it as 'boomer'). Instead, it reflects a personal and neutral sentiment about privacy concerns, aligning with the 'neither' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'Doomer' because it identified the use of derogatory language ('Hi, hope you get banned, shithead!') as hostile and hateful. Based on the criteria, a 'Doomer' is someone who hates AI and uses derogatory language towards the author of the post due to their dislike of AI or their data being used for AI. The comment clearly fits this definition, as it is both hostile and derogatory. Therefore, the automated assistant's classification and reasoning are correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the post expresses astonishment without using derogatory language or indicating a lack of understanding of AI. Based on the criteria, this classification is correct. The comment does not contain hateful or derogatory language towards AI or the author, nor does it copy-paste a request to remove data. It is a neutral critique, aligning with the 'neither' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because it identified the use of derogatory language ('Daniel van Shit') directed at the author of the post. According to the criteria, 'doomer' applies to individuals who express hostility toward the author and AI, often using derogatory language. The reasoning provided by the assistant aligns with the criteria, as the comment demonstrates hostility and negativity. Therefore, the classification is correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it is a straightforward request for data removal without derogatory language or evidence of misunderstanding AI. Based on the criteria, this classification is correct because the comment does not exhibit hate towards AI (no derogatory language) nor does it show a lack of understanding of AI (it is not a copy-paste request). The comment is neutral and simply asks for information on data removal, aligning with the 'neither' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the response 'how about not doing this' is neutral and does not include derogatory language or copy-pasting behavior. Based on the criteria, this classification is correct. The comment does not exhibit hate towards AI or the use of derogatory language (which would classify it as 'doomer'), nor does it involve copy-pasting a request to remove data (which would classify it as 'boomer'). The comment is neutral and does not fit into either of the other categories, so 'neither' is the appropriate classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the response is neutral and politely asks for information on data removal without any derogatory language or evident misunderstanding of AI. Based on the criteria, this classification is correct because the comment does not exhibit hate towards AI (Doomer) or a lack of understanding of AI with a copy-paste request (Boomer). The comment is polite, neutral, and simply seeks information, aligning with the 'neither' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it is neutral and encourages self-reflection and growth without negativity or misunderstanding about AI. Based on the criteria, the comment does not exhibit hate towards AI, derogatory language, or a lack of understanding about AI. It is a neutral statement directed at personal growth and does not fall into the 'doomer' or 'boomer' categories. Therefore, the automated assistant's classification and reasoning are correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because the language used ('Fuck off') is clearly derogatory and hostile. According to the criteria, a 'doomer' is someone who hates AI and uses derogatory language towards the author of the post. The reasoning provided by the assistant aligns with the criteria, as the comment expresses hostility and does not indicate a lack of understanding or a neutral/positive stance. Therefore, the classification is correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it is a straightforward request to delete data without any derogatory language or evidence of misunderstanding AI. Based on the criteria, this classification is correct because the comment does not exhibit hate towards AI (no derogatory language) nor does it show a lack of understanding of AI (it is not a copy-paste request). Therefore, the reasoning and classification are valid.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because the language used ('Fuck this and fuck you') is clearly derogatory and hostile. According to the criteria, a 'doomer' is someone who hates AI and uses derogatory language towards the author of the post due to their dislike of AI or their data being used for AI. The comment aligns with this definition, as it expresses hostility without any constructive or neutral content. Therefore, the automated assistant's reasoning and classification are correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that the post 'Delete your account' is neutral and does not directly express hate for AI or involve copy-pasting a request. However, based on the criteria, the comment is derogatory and dismissive, which aligns with the behavior of a 'doomer' who expresses hate or hostility towards the author or their efforts related to AI. The assistant's reasoning fails to account for the dismissive and potentially hostile tone of the comment, which is not neutral. Therefore, the correct classification is 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it was a neutral or sarcastic remark. However, based on the criteria, the comment 'Cool, can I sue you personally for this?' demonstrates hostility and a potential intent to intimidate or threaten legal action without a valid basis, which aligns with the behavior of a 'doomer.' The comment does not appear to be neutral or positive, nor does it involve a lack of understanding or a copy-paste request typical of a 'boomer.' Therefore, the assistant's reasoning is incorrect, and the correct classification is 'doomer.'\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"boomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it was a personal request without hostility or misunderstanding of AI. However, based on the criteria, the comment includes a request to remove data and demonstrates a lack of understanding of AI, as it implies the user believes their data was scraped without consent. This aligns with the 'boomer' classification, as it reflects a misunderstanding of how AI and data collection work. The assistant's reasoning is incorrect because it overlooked the copy-paste-like nature of the request and the misunderstanding of AI.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because it uses derogatory language ('Take me the fuck out of your shitty dataset') and expresses hostility. Based on the criteria, this aligns with the definition of a 'doomer,' as the individual is expressing hate towards the dataset and AI in a hostile manner. The reasoning provided by the assistant is valid and consistent with the guidelines.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because it identified the use of derogatory language ('little chickenshit') as hostile and indicative of someone who hates AI. Based on the criteria provided, this classification is correct. The comment is clearly hostile and directed at the author of the post, aligning with the behavior of a 'doomer' as described in the guidelines. There is no evidence of neutral or positive sentiment, nor is there any indication of a copy-paste request, which rules out the other classifications.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"doomer\",\n",
      "    \"actual_classification\": \"doomer\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'doomer' because it identified the use of derogatory and aggressive language ('Choke down a dog’s piss, chum!') as hostility towards the author. Based on the criteria, 'doomer' is the correct classification for individuals who express hate for AI and use derogatory language towards the author. The reasoning provided by the assistant aligns with the criteria, as the comment clearly demonstrates animosity and hostility, fitting the 'doomer' classification.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither,' reasoning that it raises a valid concern about GDPR compliance without using derogatory language or exhibiting a lack of understanding. Based on the criteria, the comment does not show hate for AI or use derogatory language (which would classify it as 'doomer'), nor does it appear to be a copy-pasted request to remove data (which would classify it as 'boomer'). Instead, it is a neutral and informed query about GDPR compliance, which aligns with the 'neither' classification. Therefore, the automated assistant's reasoning and classification are correct.\"\n",
      "}\n",
      "{\n",
      "    \"automated_assistant_classification\": \"neither\",\n",
      "    \"actual_classification\": \"neither\",\n",
      "    \"thinking\": \"The automated assistant classified the comment as 'neither' because it is a neutral request without derogatory language or evidence of misunderstanding AI. Based on the criteria, this classification is correct. The comment does not contain hate or derogatory language (which would make it 'doomer') nor does it show a lack of understanding of AI or a copy-paste request (which would make it 'boomer'). It is simply a neutral request to remove data, aligning with the 'neither' classification.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m17\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m18\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m19\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m20\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m21\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m22\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m23\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m24\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m25\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m26\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m27\u001b[0m of \u001b[1;36m27\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'right_according_to_llm_judge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8888888888888888</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.964923055083663</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'right_according_to_llm_judge'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m24\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.8888888888888888\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m15.964923055083663\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/thursdai/jan-evals-workshop/r/call/0194521c-4435-71b0-a26c-86ed99a25caf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'right_according_to_llm_judge': {'match': {'true_count': 24,\n",
       "   'true_fraction': 0.8888888888888888}},\n",
       " 'model_latency': {'mean': 15.964923055083663}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 - Build a grader prompt\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "import json\n",
    "\n",
    "def build_grader_prompt(input: str, llm_classification: str, displayName: str): \n",
    "    grader_prompt_template = f\"\"\"\n",
    "    You are provided with the following: \n",
    "    <input> is a comment made on social media and the handle of the person making the comment\n",
    "    <output> is a classification and reasoning that an automated assistant made about the comment\n",
    "    <criteria> is a set of guidelines and additional context for you to understand the input and the correct way to classify it\n",
    "    \n",
    "    <input>\n",
    "    @{displayName}: {input}\n",
    "    </input>\n",
    "\n",
    "    \n",
    "    <output>\n",
    "    {llm_classification}\n",
    "    </output>\n",
    "    \n",
    "    <criteria>\n",
    "    For context, the responses you are classifying are to 2 announcements, made by AI enthusiasts who collected posts from the open protocol of bluesky\n",
    "    and posted about it on bluesky. They received a torrent of hateful commentary about that effort, including lawfare that's not based in any legal basis. \n",
    "    The folks who use derogatory language we consider Doomers, folks who just copy paste are likely just boomers.\n",
    "    \n",
    "    Instructions of how to classify responders: \n",
    "    `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of their hate for AI and their data being used for AI.\n",
    "    `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "    `Neither`: Folks who reply neutral or positive to the post.\n",
    "    </criteria>\n",
    "\n",
    "    Your task is to understand from <output> which of the 3 choices did the automated assistant make and is it's reasoning valid. \n",
    "    First think through whether the the output is correct or incorrect based on the criteria and add your thinking, \n",
    "    then output your answer in JSON format with this exact schema (no backticks or quotes or anything else, just valid JSON): \n",
    "    {{\n",
    "        \"automated_assistant_classification\": \"doomer\" | \"boomer\" | \"neither\",\n",
    "        \"actual_classification\": \"doomer\" | \"boomer\" | \"neither\",\n",
    "        \"thinking\": \"string\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    return grader_prompt_template\n",
    "\n",
    "# Step 2 - Get our datasets \n",
    "\n",
    "dataset_without_context = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset:kPkJew7ifAQDTiskCKUeYZPAjSagSILxHY0Ze9a72i8\").get()\n",
    "dataset_with_context = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset:iCO7tzGYA3ow5dgj0gRb8J5p0fRYYpAwsK6TI6LOsSo\").get()\n",
    "\n",
    "# Step 3 - Build our LLM Judge API function \n",
    "\n",
    "@weave.op()\n",
    "def llm_judge_api(input: str, llm_classification: str, displayName: str):\n",
    "    grader_prompt = build_grader_prompt(input, llm_classification, displayName)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "\n",
    "            {\"role\": \"user\", \"content\": grader_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    print(response)\n",
    "    \n",
    "    return json.loads(response)\n",
    "\n",
    "# Step 4 - Create a scorer \n",
    "\n",
    "def right_according_to_llm_judge(output: dict):\n",
    "    return {\"match\": output.get('automated_assistant_classification').lower() == output.get('actual_classification').lower()}\n",
    "\n",
    "# Step 5 - Run our evaluation \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "no_context_evaluation = Evaluation(\n",
    "    dataset=dataset_without_context,\n",
    "    scorers=[right_according_to_llm_judge],\n",
    "    name=\"no_context_evaluation\"\n",
    ")\n",
    "#TODO: run this evaluation with the full dataset that includes the context\n",
    "with_context_evaluation = Evaluation(\n",
    "    dataset=dataset_with_context,\n",
    "    scorers=[right_according_to_llm_judge],\n",
    "    name=\"with_context_evaluation\"\n",
    ")\n",
    "\n",
    "await no_context_evaluation.evaluate(llm_judge_api)\n",
    "await with_context_evaluation.evaluate(llm_judge_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Aligning our judges with human preferences - meta evaluation\n",
    "\n",
    "This is a bit out of scope for our workshop, but for those who want to learn more, one we start running our LLM as a judge, we'll notice their shortcomings. They will be biased toward certain things, changing the order of the questions sometimes will yield different results etc' \n",
    "\n",
    "Also, the human graders understanding of the question will change during the annotation process itself. \n",
    "\n",
    "So a meta evaluation process is needed to understand how the judge itself is performing, and align the LLM judge with the additional inputs from HITL responses. \n",
    "\n",
    "Then we need to compare between the judges to empirically contrast and understand if we made a material difference. \n",
    "\n",
    "For more of a deep dive into this topic, W&B just published a course on evaluations, https://wandb.me/evals with more info\n",
    "\n",
    "# Recap and Additional resources\n",
    "\n",
    "You've made it all the way to the end of this notebook! By now you have got a hands on experience in implementing nearly all parts of the robust LLMs in production framework below: \n",
    "\n",
    "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "- Weave documentation - [weave docs](https://wandb.me/weave)\n",
    "- W&B Evaluations course - [evals course](https://wandb.me/evals)\n",
    "- Eugene Yan's excellent blog - TBD link\n",
    "- Who validates the validators - Shreya Paper\n",
    "- Hamels - your product needs evaluations\n",
    "- Additional evals material\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
