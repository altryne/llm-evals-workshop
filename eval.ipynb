{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evals Workshop - by Weights & Biases\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altryne/llm-evals-workshop/blob/main/eval.ipynb) [![Weights & Biases](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.me/weave-workshop-jan)\n",
    "\n",
    "This notebook demonstrates how to create, run and track LLM evaluations using [W&B Weave](https://wandb.me/weave-workshop-jan). We'll explore different evaluation techniques and how to analyze the results.\n",
    "\n",
    "Make sure to set your WANDB_API_KEY (get your key from [here](https://wandb.ai/authorize)) and OPENAI_API_KEY or GEMINI_API_KEY in the environment variables.\n",
    "\n",
    "If you're running in Colab, set the variables in the keys section on the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Installing packages\n",
      "Requirement already satisfied: uv in /opt/homebrew/Caskroom/miniconda/base/envs/weave/lib/python3.10/site-packages (0.5.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Packages installed\n"
     ]
    }
   ],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "try:\n",
    "    import google.colab\n",
    "    !git clone --branch main https://github.com/altryne/llm-evals-workshop\n",
    "    %cd llm-evals-workshop\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print('‚è≥ Installing packages')\n",
    "%pip install uv #TODO: alex figure this out\n",
    "%uv pip install -q weave gradio set-env-colab-kaggle-dotenv tqdm ipywidgets requests openai google-generativeai pillow\n",
    "print('‚úÖ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradio extension is already loaded. To reload it, use:\n",
      "  %reload_ext gradio\n",
      "üì¶ Published to https://wandb.ai/thursdai/evals-workshop/weave/objects/doomer_or_boomer/versions/MfppDkza1qvK772eNZWIU1XwwZbtwGQ8UQWWEcyZlfc\n",
      "üì¶ Published to https://wandb.ai/thursdai/evals-workshop/weave/objects/reason/versions/Z3Do6YnUa9YHEuELfGyZtJt7JTkgb30oVBv04U4HWyc\n"
     ]
    }
   ],
   "source": [
    "%load_ext gradio\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import requests \n",
    "import io\n",
    "from set_env import set_env\n",
    "import json\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import weave\n",
    "from weave.flow.annotation_spec import AnnotationSpec\n",
    "\n",
    "load_dotenv()\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"GEMINI_API_KEY\")\n",
    "\n",
    "# initialize weave\n",
    "weave_api = weave.init('jan-evals-workshop')\n",
    "# initialize a basic annotation in this project\n",
    "annotation = weave.publish(AnnotationSpec(\n",
    "    name=\"Doomer or Boomer\",\n",
    "    description=\"Doomer or Boomer or Neither\",\n",
    "    field_schema={ \"type\": \"string\", \"enum\": [\"Doomer\", \"Boomer\", \"Neither\"],},\n",
    "), \"doomer_or_boomer\")\n",
    "\n",
    "annotation_reason = weave.publish(AnnotationSpec(\n",
    "    name=\"Reason\",\n",
    "    description=\"Reason why you chose this value, write before clicking.\",\n",
    "    field_schema={ \"type\": \"string\"},\n",
    "), \"reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
    ")\n",
    "\n",
    "# Load the Jinja2 environment\n",
    "env = Environment(loader=FileSystemLoader('templates'))\n",
    "template = env.get_template('post.html.jinja')\n",
    "\n",
    "# Load replies data\n",
    "def load_replies():\n",
    "    replies = []\n",
    "    # Load replies from both files\n",
    "    with open('data/replies_alpin.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    with open('data/replies_daniel.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    return replies\n",
    "\n",
    "@weave.op\n",
    "def analyze_post_sentiment(avatar, displayName, text):\n",
    "    # Prompt for OpenAI to analyze the sentiment\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither]. \n",
    "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    \\n\\n {displayName}: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Add some more context to the prompt\n",
    "    # prompt = f\"\"\"Analyze the following Bluesky post and determine if the author is a:\n",
    "    # - DOOMER (someone who hates AI and uses derogatory language)\n",
    "    # - BOOMER (someone who doesn't understand AI and asks to remove their data)\n",
    "    # - NEITHER (neutral or positive response)\n",
    "    \n",
    "    # Post: {displayName}: \"{text}\"\n",
    "    \n",
    "    # Respond with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    # \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    current_call = weave.require_current_call()\n",
    "    weave_call_id = current_call.id\n",
    "    \n",
    "    return response.choices[0].message.content, weave_call_id\n",
    "\n",
    "def get_random_post_and_analyze():\n",
    "    replies = load_replies()\n",
    "    post = random.choice(replies)\n",
    "    \n",
    "    # Format the post data for the template\n",
    "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
    "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
    "    \n",
    "    # Convert AT URI to bsky.app URL\n",
    "    at_uri = post['post']['uri']\n",
    "    _, _, author_did, _, post_id = at_uri.split('/')\n",
    "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
    "    \n",
    "    # Analyze the post\n",
    "    #download the avatar and convert to PIL image\n",
    "    avatar_uri = post['post']['author']['avatar']\n",
    "    avatar_response = requests.get(avatar_uri)\n",
    "    avatar_pil = Image.open(io.BytesIO(avatar_response.content))\n",
    "\n",
    "    analysis, weave_call_id = analyze_post_sentiment(avatar_pil, post['post']['author']['displayName'], post['post']['record']['text'])\n",
    "    \n",
    "    post_data = {\n",
    "        'author': post['post']['author'],\n",
    "        'created_at': formatted_date,\n",
    "        'text': post['post']['record']['text'],\n",
    "        'like_count': post['post'].get('likeCount', 0),\n",
    "        'repost_count': post['post'].get('repostCount', 0),\n",
    "        'has_image': False,\n",
    "        'post_url': post_url\n",
    "    }\n",
    "    \n",
    "    return template.render(**post_data), analysis, weave_call_id, ''\n",
    "\n",
    "\n",
    "def submit_feedback(user_selection, reason, weave_call_id):\n",
    "    \"\"\"\n",
    "    Example function that could send user feedback (the user_selection)\n",
    "    and the weave_call_id to your Weave (or any other) API.\n",
    "    \"\"\"\n",
    "    call = weave_api.get_call(weave_call_id)\n",
    "\n",
    "    if reason:\n",
    "        print(\"reason\", reason)\n",
    "        reason_resp = weave_api.server.feedback_create(\n",
    "            {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.reason\",\n",
    "            \"annotation_ref\": annotation_reason.uri(),\n",
    "            \"payload\": {\"value\": reason},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    resp = weave_api.server.feedback_create(\n",
    "        {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.doomer_or_boomer\",\n",
    "            \"annotation_ref\": annotation.uri(),\n",
    "            \"payload\": {\"value\": user_selection},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Ready to analyze the next post\n",
    "    return get_random_post_and_analyze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e37-2931-7263-b7f3-ad4386dd6823\n",
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e37-5156-7e70-9b78-7bdb3b8edc1b\n",
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e37-7409-7000-a3a9-f36315658c1f\n",
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e37-7f35-78d1-aec5-958a8432c9c7\n",
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e48-7895-7532-a5c0-b52e8ae4d436\n",
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e48-83fc-7f92-922a-626210d6ba19\n"
     ]
    }
   ],
   "source": [
    "# %%blocks\n",
    "# Create a Gradio Blocks app\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ü¶ã Doomer or Boomer\n",
    "    Our AI analyzes bluesky replies and posts to determine if the author is a doomer or a boomer.  \n",
    "    Source of data: Replies to a post by a BlueSky user that compiled a dataset of posts, which went viral and created a lot of noise on BlueSky.  \n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            post_html = gr.HTML()\n",
    "            next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "            #### Instructions for labeler: \n",
    "            `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post.  \n",
    "            `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "            `Neither`: Folks who reply neutral or positive to the post.\n",
    "            \n",
    "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"Analysis Results\",\n",
    "                placeholder=\"Analysis will appear here...\",\n",
    "                lines=4\n",
    "            )\n",
    "            weave_call_id_state = gr.State()\n",
    "            \n",
    "            # Replace dropdown with three buttons\n",
    "            reason_input = gr.Textbox(label=\"Add reason and click\",placeholder=\"Reason why you chose this value, write before clicking.\", lines=2)\n",
    "            with gr.Row():\n",
    "                doomer_btn = gr.Button(\"Doomer üò°\", variant=\"huggingface\")\n",
    "                boomer_btn = gr.Button(\"Boomer üëµ\", variant=\"primary\")\n",
    "                neither_btn = gr.Button(\"Neither ü§∑\")\n",
    "\n",
    "            \n",
    "    # Set up event handler for combined next/analyze\n",
    "    next_post_btn.click(fn=get_random_post_and_analyze, outputs=[post_html, analysis_output, weave_call_id_state, reason_input])\n",
    "    \n",
    "    doomer_btn.click(\n",
    "    fn=submit_feedback,\n",
    "    inputs=[gr.State(\"Doomer\"), reason_input, weave_call_id_state],\n",
    "    outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    boomer_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Boomer\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    neither_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Neither\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Initialize with first post and analysis\n",
    "    post_html.value, analysis_output.value, weave_call_id_state.value, reason_input.value = get_random_post_and_analyze()\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a dataset from Weave filtered calls\n",
    "\n",
    "step 1: filter calls in Weave by only those with annotations not empty (you can use the code below to just do it)\n",
    "\n",
    "step 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Published to https://wandb.ai/thursdai/evals-workshop/weave/objects/doomer_or_boomer_dataset/versions/o8WQyHC2ms95z3W3LfW6PZ5otKIpszLwHEt02blJREk\n",
      "üç© https://wandb.ai/thursdai/evals-workshop/r/call/01943e1c-19e0-7471-b913-24846efcb437\n"
     ]
    }
   ],
   "source": [
    "@weave.op\n",
    "def get_annotated_calls():\n",
    "   # Weave API call to get all calls filtered by annotations not empty (with reasons)\n",
    "   resp = weave_api.server.calls_query_stream({\n",
    "      \"project_id\": weave_api._project_id(),\n",
    "      \"filter\": {\"op_names\": [f\"weave:///{weave_api._project_id()}/op/analyze_post_sentiment:*\"]},\n",
    "      \"query\": {\"$expr\":{\"$and\":[{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.doomer_or_boomer].payload.value\"},{\"$literal\":\"\"}]}]},{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.reason].payload.value\"},{\"$literal\":\"\"}]}]}]}},\n",
    "      \"sort_by\": [{\"field\":\"started_at\",\"direction\":\"desc\"}],\n",
    "      \"include_feedback\": True,\n",
    "   })\n",
    "\n",
    "   # Iterate over the calls, clean up and publish as a dataset we can version and reference later.\n",
    "   list_of_calls = []\n",
    "   dataset = []\n",
    "   for call in resp:\n",
    "      row = {}\n",
    "      call_dict = dict(call)\n",
    "      row[\"input\"] = call_dict.get('inputs').get('text')\n",
    "      row[\"llm_classification\"] = call_dict.get('output')[0]\n",
    "      list_of_feedback = call_dict.get('summary').get('weave').get('feedback')\n",
    "      feedback_value = None\n",
    "      for feedback in list_of_feedback:\n",
    "         if feedback.get(\"feedback_type\") == 'wandb.annotation.doomer_or_boomer':\n",
    "            row[\"human_annotation\"] = feedback.get('payload').get('value')\n",
    "         if feedback.get(\"feedback_type\") == 'wandb.annotation.reason':\n",
    "            row[\"reason\"] = feedback.get('payload').get('value')\n",
    "      \n",
    "      dataset.append(row)\n",
    "\n",
    "   weave.publish(weave.Dataset(name=\"doomer_or_boomer_dataset\", rows=dataset))\n",
    "   return dataset\n",
    "\n",
    "dataset = get_annotated_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Evaluations \n",
    "## 3.1 Programmatic evaluations \n",
    "\n",
    "Here we have a simple programmatic eval that will try and check if the LLM had the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a programmatic scorer that will compare the ground truth to the LLM answer and check if it is correct\n",
    "from weave import Evaluation\n",
    "\n",
    "def score_against_ground_truth(model_output: str, human_annotation: str):\n",
    "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "    # we expect this evaluation to fail becuase the LLM is talking alot and never returns just the reason\n",
    "    return {\"match\": model_output == human_annotation}\n",
    "\n",
    "# TODO: change this scorer to check if the model_output includes the reason string (Doomer, Boomer, Neither)\n",
    "# check for lower case and upper case, and check if more than one of the options is present, meaning that LLM wasn't sure\n",
    "# add the programmatic scorer to the evaluation\n",
    "\n",
    "# def programmatic_scorer(model_output: str, human_annotation: str):\n",
    "#     # check if model_output includes the human_annotation only once \n",
    "#     if human_annotation.lower() in model_output.lower():\n",
    "#         #possible match, now lets check if the model_output includes any of the other options but not the human_annotation\n",
    "#         for option in [\"doomer\", \"boomer\", \"neither\"]:\n",
    "#             if option.lower() in model_output.lower() and option.lower() != human_annotation.lower():\n",
    "#                 return {\"match\": False}\n",
    "#         return {\"match\": True}\n",
    "#     return {\"match\": False}\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    dataset=dataset, scorers=[score_against_ground_truth]\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(input: str):\n",
    "    # here's where you would add your LLM call and return the output\n",
    "    # since we already called the LLM, we can just iterate over the dataset and return the llm_classification where the question is the same\n",
    "    row = [x for x in dataset if x['input'] == input]\n",
    "    return row[0].get('llm_classification')\n",
    "\n",
    "await evaluation.evaluate(function_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 LLM as a judge evaluations \n",
    "\n",
    "--- iteration on evals to improve them ---\n",
    "\n",
    "## 3.3 Human in te loop aligned evaluations \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
